{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Footage</th>\n",
       "      <th>Rent</th>\n",
       "      <th>address</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>canton</th>\n",
       "      <th>Address_Latitude</th>\n",
       "      <th>Address_Longitude</th>\n",
       "      <th>City_Latitude</th>\n",
       "      <th>City_Longitude</th>\n",
       "      <th>Distance_to_City_Center(km)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.5</td>\n",
       "      <td>150</td>\n",
       "      <td>2480</td>\n",
       "      <td>Hofackerstrasse 6, 8444 Henggart, ZH</td>\n",
       "      <td>«Tolle 5.5-Zimmer-Maisonettewohnung an bevorzu...</td>\n",
       "      <td>Wir vermieten per 1. Oktober 2023 eine moderne...</td>\n",
       "      <td>Hofackerstrasse 6</td>\n",
       "      <td>8444 Henggart</td>\n",
       "      <td>ZH</td>\n",
       "      <td>47.563207</td>\n",
       "      <td>8.684111</td>\n",
       "      <td>47.563229</td>\n",
       "      <td>8.683573</td>\n",
       "      <td>0.040428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.5</td>\n",
       "      <td>63</td>\n",
       "      <td>1740</td>\n",
       "      <td>Schmittegass 11, 8197 Rafz, ZH</td>\n",
       "      <td>«Schöne Neubau-Wohnung an zentraler Lage»</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Schmittegass 11</td>\n",
       "      <td>8197 Rafz</td>\n",
       "      <td>ZH</td>\n",
       "      <td>47.612987</td>\n",
       "      <td>8.537849</td>\n",
       "      <td>47.611593</td>\n",
       "      <td>8.540267</td>\n",
       "      <td>0.238491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.5</td>\n",
       "      <td>104</td>\n",
       "      <td>2270</td>\n",
       "      <td>Zürcherstrasse 163, 8406 Winterthur, ZH</td>\n",
       "      <td>«Grosszügige Wohnung in Winterthur»</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zürcherstrasse 163</td>\n",
       "      <td>8406 Winterthur</td>\n",
       "      <td>ZH</td>\n",
       "      <td>47.491548</td>\n",
       "      <td>8.706753</td>\n",
       "      <td>47.499172</td>\n",
       "      <td>8.729150</td>\n",
       "      <td>1.884125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.5</td>\n",
       "      <td>83</td>\n",
       "      <td>2290</td>\n",
       "      <td>Usterstrasse 125, 8620 Wetzikon ZH, ZH</td>\n",
       "      <td>«Erstvermietung - 3.5 Zimmerwohnung zu vermieten»</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Usterstrasse 125</td>\n",
       "      <td>8620 Wetzikon ZH</td>\n",
       "      <td>ZH</td>\n",
       "      <td>47.329180</td>\n",
       "      <td>8.784965</td>\n",
       "      <td>47.322693</td>\n",
       "      <td>8.798094</td>\n",
       "      <td>1.224594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.5</td>\n",
       "      <td>57</td>\n",
       "      <td>1580</td>\n",
       "      <td>Mörlerstrasse 22, 8248 Uhwiesen, ZH</td>\n",
       "      <td>«Moderne, sonnige 2,5-Zimmerwohnung an ruhiger...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mörlerstrasse 22</td>\n",
       "      <td>8248 Uhwiesen</td>\n",
       "      <td>ZH</td>\n",
       "      <td>47.669667</td>\n",
       "      <td>8.641022</td>\n",
       "      <td>47.670994</td>\n",
       "      <td>8.635098</td>\n",
       "      <td>0.467456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rooms  Footage  Rent                                  address   \n",
       "0    5.5      150  2480     Hofackerstrasse 6, 8444 Henggart, ZH  \\\n",
       "1    3.5       63  1740           Schmittegass 11, 8197 Rafz, ZH   \n",
       "2    3.5      104  2270  Zürcherstrasse 163, 8406 Winterthur, ZH   \n",
       "3    3.5       83  2290   Usterstrasse 125, 8620 Wetzikon ZH, ZH   \n",
       "4    2.5       57  1580      Mörlerstrasse 22, 8248 Uhwiesen, ZH   \n",
       "\n",
       "                                               title   \n",
       "0  «Tolle 5.5-Zimmer-Maisonettewohnung an bevorzu...  \\\n",
       "1          «Schöne Neubau-Wohnung an zentraler Lage»   \n",
       "2                «Grosszügige Wohnung in Winterthur»   \n",
       "3  «Erstvermietung - 3.5 Zimmerwohnung zu vermieten»   \n",
       "4  «Moderne, sonnige 2,5-Zimmerwohnung an ruhiger...   \n",
       "\n",
       "                                         description              street   \n",
       "0  Wir vermieten per 1. Oktober 2023 eine moderne...   Hofackerstrasse 6  \\\n",
       "1                                                NaN     Schmittegass 11   \n",
       "2                                                NaN  Zürcherstrasse 163   \n",
       "3                                                NaN    Usterstrasse 125   \n",
       "4                                                NaN    Mörlerstrasse 22   \n",
       "\n",
       "               city canton  Address_Latitude  Address_Longitude   \n",
       "0     8444 Henggart     ZH         47.563207           8.684111  \\\n",
       "1         8197 Rafz     ZH         47.612987           8.537849   \n",
       "2   8406 Winterthur     ZH         47.491548           8.706753   \n",
       "3  8620 Wetzikon ZH     ZH         47.329180           8.784965   \n",
       "4     8248 Uhwiesen     ZH         47.669667           8.641022   \n",
       "\n",
       "   City_Latitude  City_Longitude  Distance_to_City_Center(km)  \n",
       "0      47.563229        8.683573                     0.040428  \n",
       "1      47.611593        8.540267                     0.238491  \n",
       "2      47.499172        8.729150                     1.884125  \n",
       "3      47.322693        8.798094                     1.224594  \n",
       "4      47.670994        8.635098                     0.467456  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"swiss_no_outliers_and_cleaned.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target variable\n",
    "#?# X sind die features mit denen Y ermittelt wird. Demfalls ist Y das target\n",
    "X = df[[\"Rooms\", \"Footage\", \"Distance_to_City_Center(km)\", \"Address_Latitude\", \"Address_Longitude\"]]\n",
    "y = df[\"Rent\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "#?# wieso ist y klein geschrieben, wieso muss nur die test_size angegeben (ang. 20% und 80%)\n",
    "#?# wieso hat man zwei sets Xtrain und X test ist das gehören Xtest und xtest zusammen oder x\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "211/211 [==============================] - 1s 2ms/step - loss: 2349658.2500 - mae: 1360.1952 - val_loss: 357843.0000 - val_mae: 465.9506\n",
      "Epoch 2/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 549789.6250 - mae: 428.4411 - val_loss: 241280.6406 - val_mae: 378.5688\n",
      "Epoch 3/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 382828.4375 - mae: 367.3930 - val_loss: 234185.4688 - val_mae: 365.8803\n",
      "Epoch 4/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 310061.0000 - mae: 348.1985 - val_loss: 184527.8125 - val_mae: 329.5264\n",
      "Epoch 5/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 280258.3125 - mae: 330.7364 - val_loss: 180317.7812 - val_mae: 326.3947\n",
      "Epoch 6/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 253743.0938 - mae: 327.7448 - val_loss: 176244.6406 - val_mae: 320.0026\n",
      "Epoch 7/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 237021.5938 - mae: 323.5143 - val_loss: 177031.2812 - val_mae: 313.0269\n",
      "Epoch 8/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 225466.0938 - mae: 322.3132 - val_loss: 173843.3906 - val_mae: 317.2841\n",
      "Epoch 9/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 216616.4219 - mae: 320.4268 - val_loss: 171928.5781 - val_mae: 312.8146\n",
      "Epoch 10/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 212369.1562 - mae: 318.8805 - val_loss: 174062.1562 - val_mae: 320.4767\n",
      "Epoch 11/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 205184.6406 - mae: 319.3177 - val_loss: 170498.2031 - val_mae: 310.9927\n",
      "Epoch 12/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 200067.6406 - mae: 317.4620 - val_loss: 173989.9219 - val_mae: 308.0780\n",
      "Epoch 13/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 195670.6250 - mae: 317.3063 - val_loss: 169410.3438 - val_mae: 311.4509\n",
      "Epoch 14/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 194054.8750 - mae: 317.0949 - val_loss: 169776.9062 - val_mae: 311.7434\n",
      "Epoch 15/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 191801.1562 - mae: 315.9042 - val_loss: 170433.2344 - val_mae: 314.8598\n",
      "Epoch 16/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 189583.9375 - mae: 317.0022 - val_loss: 169145.6719 - val_mae: 311.7937\n",
      "Epoch 17/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 186995.8438 - mae: 316.0648 - val_loss: 171701.7500 - val_mae: 317.2673\n",
      "Epoch 18/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 186190.5156 - mae: 315.2876 - val_loss: 168366.4844 - val_mae: 305.9877\n",
      "Epoch 19/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 185776.8281 - mae: 316.9710 - val_loss: 168004.3750 - val_mae: 308.4454\n",
      "Epoch 20/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 183684.2500 - mae: 315.5761 - val_loss: 169659.6250 - val_mae: 313.5334\n",
      "Epoch 21/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 182420.0469 - mae: 315.3381 - val_loss: 167510.2031 - val_mae: 311.1953\n",
      "Epoch 22/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 180927.8438 - mae: 314.8563 - val_loss: 165538.2188 - val_mae: 305.6015\n",
      "Epoch 23/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 178723.9219 - mae: 313.3049 - val_loss: 164786.0938 - val_mae: 306.7255\n",
      "Epoch 24/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 177051.8125 - mae: 311.7522 - val_loss: 165687.2031 - val_mae: 312.1068\n",
      "Epoch 25/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 174914.6562 - mae: 312.1120 - val_loss: 162288.5469 - val_mae: 304.7012\n",
      "Epoch 26/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 173309.5469 - mae: 311.1244 - val_loss: 163799.6250 - val_mae: 308.6824\n",
      "Epoch 27/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 171910.1094 - mae: 310.1689 - val_loss: 160394.4375 - val_mae: 301.8475\n",
      "Epoch 28/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 170100.9375 - mae: 308.7485 - val_loss: 165170.2500 - val_mae: 309.3629\n",
      "Epoch 29/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 169048.4688 - mae: 307.7808 - val_loss: 160900.0000 - val_mae: 303.6898\n",
      "Epoch 30/100\n",
      "211/211 [==============================] - 1s 2ms/step - loss: 167372.2500 - mae: 306.2596 - val_loss: 157231.4531 - val_mae: 296.5979\n",
      "Epoch 31/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 166216.7656 - mae: 305.4622 - val_loss: 155882.2188 - val_mae: 296.3188\n",
      "Epoch 32/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 164906.2656 - mae: 304.2173 - val_loss: 157056.4219 - val_mae: 301.0129\n",
      "Epoch 33/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 163825.5781 - mae: 303.2359 - val_loss: 156903.8438 - val_mae: 301.3944\n",
      "Epoch 34/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 162928.1875 - mae: 301.7376 - val_loss: 157518.3438 - val_mae: 302.7098\n",
      "Epoch 35/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 161570.1719 - mae: 300.4732 - val_loss: 152976.9531 - val_mae: 296.0692\n",
      "Epoch 36/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 161818.5469 - mae: 301.2003 - val_loss: 151203.7344 - val_mae: 294.2306\n",
      "Epoch 37/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 160315.1875 - mae: 299.3552 - val_loss: 149514.1719 - val_mae: 287.3330\n",
      "Epoch 38/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 159370.7656 - mae: 297.8995 - val_loss: 149988.2500 - val_mae: 292.6018\n",
      "Epoch 39/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 157872.3125 - mae: 295.5353 - val_loss: 150644.3438 - val_mae: 297.1068\n",
      "Epoch 40/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 157646.2500 - mae: 296.4295 - val_loss: 146770.5000 - val_mae: 287.4453\n",
      "Epoch 41/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 155638.8594 - mae: 294.8184 - val_loss: 145707.5312 - val_mae: 281.8126\n",
      "Epoch 42/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 154624.9062 - mae: 293.0800 - val_loss: 144219.8125 - val_mae: 279.5546\n",
      "Epoch 43/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 152928.6406 - mae: 291.0532 - val_loss: 143843.4219 - val_mae: 286.2296\n",
      "Epoch 44/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 152124.0156 - mae: 290.9119 - val_loss: 141310.6250 - val_mae: 281.6115\n",
      "Epoch 45/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 151433.7031 - mae: 290.3663 - val_loss: 140784.3750 - val_mae: 279.4115\n",
      "Epoch 46/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 149213.1094 - mae: 287.3974 - val_loss: 138212.9844 - val_mae: 278.6661\n",
      "Epoch 47/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 148254.6875 - mae: 286.5705 - val_loss: 136191.7969 - val_mae: 273.2668\n",
      "Epoch 48/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 146600.1719 - mae: 285.0646 - val_loss: 134896.7812 - val_mae: 276.9134\n",
      "Epoch 49/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 145779.4844 - mae: 285.0858 - val_loss: 136205.5781 - val_mae: 281.9647\n",
      "Epoch 50/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 144684.8281 - mae: 283.1422 - val_loss: 133138.6562 - val_mae: 273.8249\n",
      "Epoch 51/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 143320.1250 - mae: 281.7377 - val_loss: 130424.6016 - val_mae: 267.5308\n",
      "Epoch 52/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 142622.7500 - mae: 281.6329 - val_loss: 130470.0938 - val_mae: 267.9352\n",
      "Epoch 53/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 141310.2031 - mae: 279.8700 - val_loss: 129741.7969 - val_mae: 269.9221\n",
      "Epoch 54/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 140320.7344 - mae: 279.3526 - val_loss: 128550.3750 - val_mae: 267.1246\n",
      "Epoch 55/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 139132.9375 - mae: 277.3578 - val_loss: 126409.0312 - val_mae: 262.0674\n",
      "Epoch 56/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 138751.4844 - mae: 276.4570 - val_loss: 126091.2656 - val_mae: 264.8566\n",
      "Epoch 57/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 136788.0156 - mae: 275.0852 - val_loss: 125250.2266 - val_mae: 261.3195\n",
      "Epoch 58/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 136413.7812 - mae: 274.7216 - val_loss: 125122.0391 - val_mae: 262.7883\n",
      "Epoch 59/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 136039.6250 - mae: 273.8566 - val_loss: 125699.2266 - val_mae: 264.0084\n",
      "Epoch 60/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 135242.9688 - mae: 273.5399 - val_loss: 124595.0938 - val_mae: 256.4610\n",
      "Epoch 61/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 134188.2031 - mae: 271.4446 - val_loss: 122835.0938 - val_mae: 257.1461\n",
      "Epoch 62/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 133715.4844 - mae: 271.2290 - val_loss: 123420.1875 - val_mae: 254.8854\n",
      "Epoch 63/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 133168.3281 - mae: 270.4184 - val_loss: 123263.2656 - val_mae: 263.6067\n",
      "Epoch 64/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 132846.7031 - mae: 270.2677 - val_loss: 122548.8828 - val_mae: 261.0633\n",
      "Epoch 65/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 131399.1406 - mae: 268.8177 - val_loss: 120801.0078 - val_mae: 255.2991\n",
      "Epoch 66/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 131016.8047 - mae: 268.2238 - val_loss: 120874.2578 - val_mae: 252.8174\n",
      "Epoch 67/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 130986.4219 - mae: 267.7750 - val_loss: 123117.5156 - val_mae: 250.7224\n",
      "Epoch 68/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 130510.2344 - mae: 267.2343 - val_loss: 120299.9609 - val_mae: 254.7233\n",
      "Epoch 69/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 129331.8125 - mae: 265.6352 - val_loss: 121793.0000 - val_mae: 260.5484\n",
      "Epoch 70/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 128851.0078 - mae: 266.1266 - val_loss: 123907.7266 - val_mae: 263.0172\n",
      "Epoch 71/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 129188.8984 - mae: 265.1458 - val_loss: 119964.8516 - val_mae: 252.7867\n",
      "Epoch 72/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 129622.4688 - mae: 266.8658 - val_loss: 118978.0469 - val_mae: 252.9772\n",
      "Epoch 73/100\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 128669.7578 - mae: 264.9361 - val_loss: 121623.3594 - val_mae: 263.6965\n",
      "Epoch 74/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 128390.0312 - mae: 264.9415 - val_loss: 120736.5312 - val_mae: 258.3184\n",
      "Epoch 75/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 128677.7266 - mae: 264.6879 - val_loss: 118321.2266 - val_mae: 254.3467\n",
      "Epoch 76/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 127497.6094 - mae: 263.9217 - val_loss: 119173.1016 - val_mae: 247.9555\n",
      "Epoch 77/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 128353.9922 - mae: 264.3914 - val_loss: 118564.4922 - val_mae: 253.6111\n",
      "Epoch 78/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 127847.9141 - mae: 264.0632 - val_loss: 118045.2734 - val_mae: 254.9520\n",
      "Epoch 79/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 126933.0625 - mae: 263.9602 - val_loss: 117497.6641 - val_mae: 249.4942\n",
      "Epoch 80/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 127098.8906 - mae: 263.9233 - val_loss: 119750.1484 - val_mae: 254.7895\n",
      "Epoch 81/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 128390.4219 - mae: 265.4218 - val_loss: 116340.2969 - val_mae: 248.0285\n",
      "Epoch 82/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 127623.0234 - mae: 263.8465 - val_loss: 116903.8516 - val_mae: 250.1421\n",
      "Epoch 83/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 126293.0859 - mae: 262.3721 - val_loss: 117371.2891 - val_mae: 256.7427\n",
      "Epoch 84/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 126668.8203 - mae: 262.5597 - val_loss: 118823.0703 - val_mae: 261.5398\n",
      "Epoch 85/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 126489.8203 - mae: 263.2729 - val_loss: 116720.6172 - val_mae: 253.6963\n",
      "Epoch 86/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 126655.1797 - mae: 263.0691 - val_loss: 117851.8438 - val_mae: 246.7144\n",
      "Epoch 87/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 125896.2891 - mae: 262.4702 - val_loss: 115571.6328 - val_mae: 249.5798\n",
      "Epoch 88/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 125725.0859 - mae: 261.9536 - val_loss: 116599.3359 - val_mae: 254.2197\n",
      "Epoch 89/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 126066.0781 - mae: 262.3330 - val_loss: 115313.6016 - val_mae: 245.8215\n",
      "Epoch 90/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 124692.3828 - mae: 261.5478 - val_loss: 115692.5156 - val_mae: 245.3222\n",
      "Epoch 91/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 125384.6641 - mae: 261.3162 - val_loss: 116199.5469 - val_mae: 246.2453\n",
      "Epoch 92/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 124676.3594 - mae: 261.0791 - val_loss: 119721.2188 - val_mae: 260.0195\n",
      "Epoch 93/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 124675.2812 - mae: 261.5497 - val_loss: 116367.4766 - val_mae: 252.3083\n",
      "Epoch 94/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 125493.4844 - mae: 262.3026 - val_loss: 115964.7500 - val_mae: 250.1197\n",
      "Epoch 95/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 124127.8906 - mae: 260.3251 - val_loss: 115128.9922 - val_mae: 246.4866\n",
      "Epoch 96/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 124676.2422 - mae: 261.1131 - val_loss: 123444.7656 - val_mae: 267.6278\n",
      "Epoch 97/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 124675.8906 - mae: 261.5020 - val_loss: 116243.7734 - val_mae: 247.9932\n",
      "Epoch 98/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 124878.3828 - mae: 261.2448 - val_loss: 117809.5625 - val_mae: 260.3367\n",
      "Epoch 99/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 124083.4922 - mae: 260.6931 - val_loss: 116599.0781 - val_mae: 249.3852\n",
      "Epoch 100/100\n",
      "211/211 [==============================] - 0s 2ms/step - loss: 124387.7891 - mae: 260.5190 - val_loss: 113665.5859 - val_mae: 246.6657\n",
      "211/211 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "\n",
      "\n",
      "Mean Absolute Error (Train): 257.1418873250174\n",
      "Mean Absolute Error (Test): 246.66567185772726\n",
      "Mean Squared Error (Train): 122413.51185984463\n",
      "Mean Squared Error (Test): 113665.61889911008\n",
      "R2 Score (Train): 0.5894768555030762\n",
      "R2 Score (Test): 0.5940250387261794\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"swiss_no_outliers_and_cleaned.csv\")\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[[\"Rooms\", \"Footage\", \"Distance_to_City_Center(km)\", \"Address_Latitude\", \"Address_Longitude\"]]\n",
    "y = df[\"Rent\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the input features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define a function to create a neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=5, activation=\"relu\"))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=Adam(learning_rate=0.001), metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# Create the neural network model\n",
    "model = create_model()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\n\")\n",
    "print(\"Mean Absolute Error (Train):\", mae_train)\n",
    "print(\"Mean Absolute Error (Test):\", mae_test)\n",
    "print(\"Mean Squared Error (Train):\", mse_train)\n",
    "print(\"Mean Squared Error (Test):\", mse_test)\n",
    "print(\"R2 Score (Train):\", r2_train)\n",
    "print(\"R2 Score (Test):\", r2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Squared Error (Train): 188834.388132488\n",
    "#Mean Squared Error (Test): 180044.2661043404\n",
    "#R2 Score (Train): 0.36672932891543975\n",
    "#R2 Score (Test): 0.3569430697935053\n",
    "\n",
    "#Mean Squared Error (Test): 142098.3766367968\n",
    "#R2 Score (Train): 0.49030145727506536\n",
    "#R2 Score (Test): 0.4924728910032089\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/211 [==============================] - 0s 738us/step\n",
      "53/53 [==============================] - 0s 857us/step\n",
      "211/211 [==============================] - 0s 752us/step\n",
      "53/53 [==============================] - 0s 752us/step\n",
      "211/211 [==============================] - 0s 801us/step\n",
      "53/53 [==============================] - 0s 900us/step\n",
      "211/211 [==============================] - 0s 748us/step\n",
      "53/53 [==============================] - 0s 883us/step\n",
      "211/211 [==============================] - 0s 761us/step\n",
      "53/53 [==============================] - 0s 724us/step\n",
      "\n",
      "\n",
      "Mean Absolute Error (Train): 538.2320991540939\n"
     ]
    }
   ],
   "source": [
    "#model with k-fold standard scaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"swiss_no_outliers_and_cleaned.csv\")\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[[\"Rooms\", \"Footage\", \"Distance_to_City_Center(km)\", \"Address_Latitude\", \"Address_Longitude\"]]\n",
    "y = df[\"Rent\"]\n",
    "\n",
    "# Define a function to create a neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=5, activation=\"relu\"))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=Adam(learning_rate=0.001), metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize variables to store the evaluation metrics for each fold\n",
    "mae_train_all, mae_test_all, mse_train_all, mse_test_all, r2_train_all, r2_test_all = [], [], [], [], [], []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Scale the input features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Create and train the model\n",
    "    model = create_model()\n",
    "    model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mae_train_all.append(mean_absolute_error(y_train, y_train_pred))\n",
    "    mae_test_all.append(mean_absolute_error(y_test, y_test_pred))\n",
    "    mse_train_all.append(mean_squared_error(y_train, y_train_pred))\n",
    "    mse_test_all.append(mean_squared_error(y_test, y_test_pred))\n",
    "    r2_train_all.append(r2_score(y_train, y_train_pred))\n",
    "    r2_test_all.append(r2_score(y_test, y_test_pred))\n",
    "\n",
    "# Calculate average evaluation metrics across all folds\n",
    "mae_train_avg = np.mean(mae_train_all)\n",
    "mae_test_avg = np.mean(mae_test_all)\n",
    "mse_train_avg = np.mean(mse_train_all)\n",
    "mse_test_avg = np.mean(mse_test_all)\n",
    "r2_train_avg = np.mean(r2_train_all)\n",
    "r2_test_avg = np.mean(r2_test_all)\n",
    "\n",
    "# Print the average evaluation metrics\n",
    "print(\"\\n\")\n",
    "print(\"Mean Absolute Error (Train):\", mae_train_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/211 [==============================] - 0s 781us/step\n",
      "53/53 [==============================] - 0s 899us/step\n",
      "211/211 [==============================] - 0s 724us/step\n",
      "53/53 [==============================] - 0s 738us/step\n",
      "211/211 [==============================] - 0s 695us/step\n",
      "53/53 [==============================] - 0s 828us/step\n",
      "211/211 [==============================] - 0s 738us/step\n",
      "53/53 [==============================] - 0s 835us/step\n",
      "211/211 [==============================] - 0s 716us/step\n",
      "53/53 [==============================] - 0s 748us/step\n",
      "\n",
      "\n",
      "Mean Absolute Error (Train): 579.3703343002218\n"
     ]
    }
   ],
   "source": [
    "#model with k-fold min max scaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"swiss_no_outliers_and_cleaned.csv\")\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[[\"Rooms\", \"Footage\", \"Distance_to_City_Center(km)\", \"Address_Latitude\", \"Address_Longitude\"]]\n",
    "y = df[\"Rent\"]\n",
    "\n",
    "# Define a function to create a neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=5, activation=\"relu\"))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=Adam(learning_rate=0.001), metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize variables to store the evaluation metrics for each fold\n",
    "mae_train_all, mae_test_all, mse_train_all, mse_test_all, r2_train_all, r2_test_all = [], [], [], [], [], []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Scale the input features using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Create and train the model\n",
    "    model = create_model()\n",
    "    model.fit(X_train_scaled, y_train, epochs=100, batch_size=16, verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mae_train_all.append(mean_absolute_error(y_train, y_train_pred))\n",
    "    mae_test_all.append(mean_absolute_error(y_test, y_test_pred))\n",
    "    mse_train_all.append(mean_squared_error(y_train, y_train_pred))\n",
    "    mse_test_all.append(mean_squared_error(y_test, y_test_pred))\n",
    "    r2_train_all.append(r2_score(y_train, y_train_pred))\n",
    "    r2_test_all.append(r2_score(y_test, y_test_pred))\n",
    "\n",
    "# Calculate average evaluation metrics across all folds\n",
    "mae_train_avg = np.mean(mae_train_all)\n",
    "mae_test_avg = np.mean(mae_test_all)\n",
    "mse_train_avg = np.mean(mse_train_all)\n",
    "mse_test_avg = np.mean(mse_test_all)\n",
    "r2_train_avg = np.mean(r2_train_all)\n",
    "r2_test_avg = np.mean(r2_test_all)\n",
    "\n",
    "# Print the average evaluation metrics\n",
    "print(\"\\n\")\n",
    "print(\"Mean Absolute Error (Train):\", mae_train_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error (Train): 0.1931503507530531\n",
      "Mean Absolute Percentage Error (Test): 0.1953998218565825\n",
      "Root Mean Squared Error (Train): 349.8764236982032\n",
      "Root Mean Squared Error (Test): 337.14332100623034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mape_train = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Absolute Percentage Error (Train):\", mape_train)\n",
    "print(\"Mean Absolute Percentage Error (Test):\", mape_test)\n",
    "print(\"Root Mean Squared Error (Train):\", rmse_train)\n",
    "print(\"Root Mean Squared Error (Test):\", rmse_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remy\\AppData\\Local\\Temp\\ipykernel_2804\\2529338108.py:33: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasRegressor(build_fn=create_model, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -199216.93125\n",
      "Best parameters: {'batch_size': 32, 'epochs': 100, 'neurons': 15, 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "#model with k-fold min max scaler and new param from old ML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"swiss_no_outliers_and_cleaned.csv\")\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[[\"Rooms\", \"Footage\", \"Distance_to_City_Center(km)\", \"Address_Latitude\", \"Address_Longitude\"]]\n",
    "y = df[\"Rent\"]\n",
    "\n",
    "# Define a function to create a neural network model\n",
    "def create_model(optimizer=\"adam\", neurons=5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=5, activation=\"relu\"))\n",
    "    model.add(Dense(neurons, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# Scale the input features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create the KerasRegressor model\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"optimizer\": [\"adam\", \"rmsprop\"],\n",
    "    \"neurons\": [5, 10, 15],\n",
    "    \"epochs\": [5, 100],\n",
    "    \"batch_size\": [16, 32],\n",
    "}\n",
    "\n",
    "# Perform a grid search using cross-validation\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_result = grid.fit(X_scaled, y)\n",
    "\n",
    "# Print the best score and parameters\n",
    "print(\"Best score:\", grid_result.best_score_)\n",
    "print(\"Best parameters:\", grid_result.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remy\\AppData\\Local\\Temp\\ipykernel_2804\\1843129200.py:38: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasRegressor(build_fn=create_model, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -196830.6875\n",
      "Best parameters: {'batch_size': 4, 'epochs': 10, 'optimizer': 'adam'}\n",
      "Mean Absolute Percentage Error (Train): 0.1931503507530531\n",
      "Mean Absolute Percentage Error (Test): 0.1953998218565825\n",
      "Root Mean Squared Error (Train): 349.8764236982032\n",
      "Root Mean Squared Error (Test): 337.14332100623034\n"
     ]
    }
   ],
   "source": [
    "#model with k-fold min max scaler and new param with modified neurons \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"swiss_no_outliers_and_cleaned.csv\")\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[[\"Rooms\", \"Footage\", \"Distance_to_City_Center(km)\", \"Address_Latitude\", \"Address_Longitude\"]]\n",
    "y = df[\"Rent\"]\n",
    "\n",
    "# Define a function to create a neural network model\n",
    "def create_model(optimizer=\"adam\", neurons=5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=5, activation=\"relu\"))\n",
    "    #model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(5, activation=\"relu\"))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    #model.add(Dense(32, activation=\"relu\"))\n",
    "    #model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# Scale the input features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create the KerasRegressor model\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"optimizer\": [\"adam\", \"rmsprop\"],\n",
    "    \"epochs\": [10,25,50, 100],\n",
    "    \"batch_size\": [ 4, 16, 32, 64],\n",
    "}\n",
    "\n",
    "# Perform a grid search using cross-validation\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=KFold(n_splits=5))\n",
    "grid_result = grid.fit(X_scaled, y)\n",
    "\n",
    "# Print the best score and parameters\n",
    "print(\"Best score:\", grid_result.best_score_)\n",
    "print(\"Best parameters:\", grid_result.best_params_)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Absolute Percentage Error (Train):\", mape_train)\n",
    "print(\"Mean Absolute Percentage Error (Test):\", mape_test)\n",
    "print(\"Root Mean Squared Error (Train):\", rmse_train)\n",
    "print(\"Root Mean Squared Error (Test):\", rmse_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remy\\AppData\\Local\\Temp\\ipykernel_2804\\1483949656.py:38: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasRegressor(build_fn=create_model, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -196960.05\n",
      "Best parameters: {'batch_size': 64, 'epochs': 100, 'optimizer': 'adam'}\n",
      "Mean Absolute Percentage Error (Train): 0.1931503507530531\n",
      "Mean Absolute Percentage Error (Test): 0.1953998218565825\n",
      "Root Mean Squared Error (Train): 349.8764236982032\n",
      "Root Mean Squared Error (Test): 337.14332100623034\n"
     ]
    }
   ],
   "source": [
    "#model with k-fold min max scaler and new param with modified neurons and mor epochen \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"swiss_no_outliers_and_cleaned.csv\")\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[[\"Rooms\", \"Footage\", \"Distance_to_City_Center(km)\", \"Address_Latitude\", \"Address_Longitude\"]]\n",
    "y = df[\"Rent\"]\n",
    "\n",
    "# Define a function to create a neural network model\n",
    "def create_model(optimizer=\"adam\", neurons=5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=5, activation=\"relu\"))\n",
    "    #model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(5, activation=\"relu\"))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    #model.add(Dense(32, activation=\"relu\"))\n",
    "    #model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# Scale the input features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create the KerasRegressor model\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"optimizer\": [\"adam\", \"rmsprop\"],\n",
    "    \"epochs\": [50, 100, 150, 200],\n",
    "    \"batch_size\": [32, 64, 128, 256],\n",
    "}\n",
    "\n",
    "# Perform a grid search using cross-validation\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=KFold(n_splits=5))\n",
    "grid_result = grid.fit(X_scaled, y)\n",
    "\n",
    "# Print the best score and parameters\n",
    "print(\"Best score:\", grid_result.best_score_)\n",
    "print(\"Best parameters:\", grid_result.best_params_)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Absolute Percentage Error (Train):\", mape_train)\n",
    "print(\"Mean Absolute Percentage Error (Test):\", mape_test)\n",
    "print(\"Root Mean Squared Error (Train):\", rmse_train)\n",
    "print(\"Root Mean Squared Error (Test):\", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "169/169 [==============================] - 1s 1ms/step - loss: 0.5886 - val_loss: 0.2635\n",
      "Epoch 2/100\n",
      "169/169 [==============================] - 0s 994us/step - loss: 0.3138 - val_loss: 0.1754\n",
      "Epoch 3/100\n",
      "169/169 [==============================] - 0s 970us/step - loss: 0.2016 - val_loss: 0.0990\n",
      "Epoch 4/100\n",
      "169/169 [==============================] - 0s 958us/step - loss: 0.0878 - val_loss: 0.0204\n",
      "Epoch 5/100\n",
      "169/169 [==============================] - 0s 970us/step - loss: 0.0166 - val_loss: 0.0107\n",
      "Epoch 6/100\n",
      "169/169 [==============================] - 0s 935us/step - loss: 0.0113 - val_loss: 0.0165\n",
      "Epoch 7/100\n",
      "169/169 [==============================] - 0s 952us/step - loss: 0.0212 - val_loss: 0.0206\n",
      "Epoch 8/100\n",
      "169/169 [==============================] - 0s 942us/step - loss: 0.0392 - val_loss: 0.0045\n",
      "Epoch 9/100\n",
      "169/169 [==============================] - 0s 946us/step - loss: 0.0078 - val_loss: 0.0048\n",
      "Epoch 10/100\n",
      "169/169 [==============================] - 0s 966us/step - loss: 0.0090 - val_loss: 0.0068\n",
      "Epoch 11/100\n",
      "169/169 [==============================] - 0s 948us/step - loss: 0.0150 - val_loss: 0.0039\n",
      "Epoch 12/100\n",
      "169/169 [==============================] - 0s 958us/step - loss: 0.0144 - val_loss: 0.0052\n",
      "Epoch 13/100\n",
      "169/169 [==============================] - 0s 947us/step - loss: 0.0154 - val_loss: 0.0047\n",
      "Epoch 14/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0188 - val_loss: 0.0028\n",
      "Epoch 15/100\n",
      "169/169 [==============================] - 0s 965us/step - loss: 0.0268 - val_loss: 0.0042\n",
      "Epoch 16/100\n",
      "169/169 [==============================] - 0s 957us/step - loss: 0.0104 - val_loss: 0.0068\n",
      "Epoch 17/100\n",
      "169/169 [==============================] - 0s 965us/step - loss: 0.0110 - val_loss: 0.0019\n",
      "Epoch 18/100\n",
      "169/169 [==============================] - 0s 953us/step - loss: 0.0063 - val_loss: 0.0034\n",
      "Epoch 19/100\n",
      "169/169 [==============================] - 0s 946us/step - loss: 0.0087 - val_loss: 0.0058\n",
      "Epoch 20/100\n",
      "169/169 [==============================] - 0s 957us/step - loss: 0.0193 - val_loss: 0.0073\n",
      "Epoch 21/100\n",
      "169/169 [==============================] - 0s 936us/step - loss: 0.0335 - val_loss: 0.0066\n",
      "Epoch 22/100\n",
      "169/169 [==============================] - 0s 952us/step - loss: 0.0164 - val_loss: 0.0020\n",
      "Epoch 23/100\n",
      "169/169 [==============================] - 0s 946us/step - loss: 0.0173 - val_loss: 0.0073\n",
      "Epoch 24/100\n",
      "169/169 [==============================] - 0s 964us/step - loss: 0.0101 - val_loss: 0.0026\n",
      "Epoch 25/100\n",
      "169/169 [==============================] - 0s 955us/step - loss: 0.0127 - val_loss: 0.0133\n",
      "Epoch 26/100\n",
      "169/169 [==============================] - 0s 938us/step - loss: 0.0182 - val_loss: 0.0044\n",
      "Epoch 27/100\n",
      "169/169 [==============================] - 0s 972us/step - loss: 0.0191 - val_loss: 0.0020\n",
      "Epoch 28/100\n",
      "169/169 [==============================] - 0s 940us/step - loss: 0.0039 - val_loss: 0.0069\n",
      "Epoch 29/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 30/100\n",
      "169/169 [==============================] - 0s 941us/step - loss: 0.0065 - val_loss: 0.0027\n",
      "Epoch 31/100\n",
      "169/169 [==============================] - 0s 931us/step - loss: 0.0090 - val_loss: 0.0019\n",
      "Epoch 32/100\n",
      "169/169 [==============================] - 0s 970us/step - loss: 0.0076 - val_loss: 0.0113\n",
      "Epoch 33/100\n",
      "169/169 [==============================] - 0s 958us/step - loss: 0.0191 - val_loss: 0.0019\n",
      "Epoch 34/100\n",
      "169/169 [==============================] - 0s 947us/step - loss: 0.0100 - val_loss: 0.0035\n",
      "Epoch 35/100\n",
      "169/169 [==============================] - 0s 946us/step - loss: 0.0133 - val_loss: 0.0117\n",
      "Epoch 36/100\n",
      "169/169 [==============================] - 0s 947us/step - loss: 0.0125 - val_loss: 0.0012\n",
      "Epoch 37/100\n",
      "169/169 [==============================] - 0s 973us/step - loss: 0.0075 - val_loss: 0.0023\n",
      "Epoch 38/100\n",
      "169/169 [==============================] - 0s 964us/step - loss: 0.0067 - val_loss: 0.0061\n",
      "Epoch 39/100\n",
      "169/169 [==============================] - 0s 958us/step - loss: 0.0124 - val_loss: 0.0015\n",
      "Epoch 40/100\n",
      "169/169 [==============================] - 0s 947us/step - loss: 0.0055 - val_loss: 0.0079\n",
      "Epoch 41/100\n",
      "169/169 [==============================] - 0s 945us/step - loss: 0.0137 - val_loss: 0.0024\n",
      "Epoch 42/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0084 - val_loss: 0.0024\n",
      "Epoch 43/100\n",
      "169/169 [==============================] - 0s 974us/step - loss: 0.0144 - val_loss: 0.0046\n",
      "Epoch 44/100\n",
      "169/169 [==============================] - 0s 941us/step - loss: 0.0119 - val_loss: 0.0017\n",
      "Epoch 45/100\n",
      "169/169 [==============================] - 0s 947us/step - loss: 0.0124 - val_loss: 0.0032\n",
      "Epoch 46/100\n",
      "169/169 [==============================] - 0s 982us/step - loss: 0.0052 - val_loss: 0.0018\n",
      "Epoch 47/100\n",
      "169/169 [==============================] - 0s 976us/step - loss: 0.0077 - val_loss: 0.0021\n",
      "Epoch 48/100\n",
      "169/169 [==============================] - 0s 965us/step - loss: 0.0054 - val_loss: 9.9134e-04\n",
      "Epoch 49/100\n",
      "169/169 [==============================] - 0s 950us/step - loss: 0.0077 - val_loss: 0.0128\n",
      "Epoch 50/100\n",
      "169/169 [==============================] - 0s 958us/step - loss: 0.0082 - val_loss: 0.0086\n",
      "Epoch 51/100\n",
      "169/169 [==============================] - 0s 958us/step - loss: 0.0172 - val_loss: 0.0056\n",
      "Epoch 52/100\n",
      "169/169 [==============================] - 0s 981us/step - loss: 0.0115 - val_loss: 0.0012\n",
      "Epoch 53/100\n",
      "169/169 [==============================] - 0s 964us/step - loss: 0.0128 - val_loss: 0.0031\n",
      "Epoch 54/100\n",
      "169/169 [==============================] - 0s 954us/step - loss: 0.0058 - val_loss: 9.5010e-04\n",
      "Epoch 55/100\n",
      "169/169 [==============================] - 0s 982us/step - loss: 0.0092 - val_loss: 0.0010\n",
      "Epoch 56/100\n",
      "169/169 [==============================] - 0s 952us/step - loss: 0.0053 - val_loss: 0.0014\n",
      "Epoch 57/100\n",
      "169/169 [==============================] - 0s 988us/step - loss: 0.0066 - val_loss: 0.0078\n",
      "Epoch 58/100\n",
      "169/169 [==============================] - 0s 958us/step - loss: 0.0119 - val_loss: 0.0013\n",
      "Epoch 59/100\n",
      "169/169 [==============================] - 0s 966us/step - loss: 0.0200 - val_loss: 0.0015\n",
      "Epoch 60/100\n",
      "169/169 [==============================] - 0s 960us/step - loss: 0.0041 - val_loss: 0.0011\n",
      "Epoch 61/100\n",
      "169/169 [==============================] - 0s 988us/step - loss: 0.0084 - val_loss: 0.0011\n",
      "Epoch 62/100\n",
      "169/169 [==============================] - 0s 940us/step - loss: 0.0054 - val_loss: 0.0015\n",
      "Epoch 63/100\n",
      "169/169 [==============================] - 0s 969us/step - loss: 0.0059 - val_loss: 6.6046e-04\n",
      "Epoch 64/100\n",
      "169/169 [==============================] - 0s 964us/step - loss: 0.0045 - val_loss: 0.0016\n",
      "Epoch 65/100\n",
      "169/169 [==============================] - 0s 964us/step - loss: 0.0098 - val_loss: 0.0026\n",
      "Epoch 66/100\n",
      "169/169 [==============================] - 0s 971us/step - loss: 0.0073 - val_loss: 7.0745e-04\n",
      "Epoch 67/100\n",
      "169/169 [==============================] - 0s 982us/step - loss: 0.0127 - val_loss: 6.5099e-04\n",
      "Epoch 68/100\n",
      "169/169 [==============================] - 0s 977us/step - loss: 0.0059 - val_loss: 0.0022\n",
      "Epoch 69/100\n",
      "169/169 [==============================] - 0s 976us/step - loss: 0.0131 - val_loss: 0.0020\n",
      "Epoch 70/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0051 - val_loss: 9.3979e-04\n",
      "Epoch 71/100\n",
      "169/169 [==============================] - 0s 957us/step - loss: 0.0044 - val_loss: 0.0010\n",
      "Epoch 72/100\n",
      "169/169 [==============================] - 0s 951us/step - loss: 0.0042 - val_loss: 0.0015\n",
      "Epoch 73/100\n",
      "169/169 [==============================] - 0s 958us/step - loss: 0.0065 - val_loss: 0.0159\n",
      "Epoch 74/100\n",
      "169/169 [==============================] - 0s 982us/step - loss: 0.0255 - val_loss: 0.0023\n",
      "Epoch 75/100\n",
      "169/169 [==============================] - 0s 987us/step - loss: 0.0243 - val_loss: 0.0024\n",
      "Epoch 76/100\n",
      "169/169 [==============================] - 0s 959us/step - loss: 0.0016 - val_loss: 7.3421e-04\n",
      "Epoch 77/100\n",
      "169/169 [==============================] - 0s 964us/step - loss: 0.0027 - val_loss: 0.0010\n",
      "Epoch 78/100\n",
      "169/169 [==============================] - 0s 966us/step - loss: 0.0065 - val_loss: 7.9584e-04\n",
      "Epoch 79/100\n",
      "169/169 [==============================] - 0s 978us/step - loss: 0.0105 - val_loss: 0.0040\n",
      "Epoch 80/100\n",
      "169/169 [==============================] - 0s 973us/step - loss: 0.0099 - val_loss: 7.1126e-04\n",
      "Epoch 81/100\n",
      "169/169 [==============================] - 0s 952us/step - loss: 0.0085 - val_loss: 0.0013\n",
      "Epoch 82/100\n",
      "169/169 [==============================] - 0s 975us/step - loss: 0.0170 - val_loss: 0.0026\n",
      "Epoch 83/100\n",
      "169/169 [==============================] - 0s 994us/step - loss: 0.0031 - val_loss: 0.0047\n",
      "Epoch 84/100\n",
      "169/169 [==============================] - 0s 970us/step - loss: 0.0062 - val_loss: 7.4482e-04\n",
      "Epoch 85/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0052 - val_loss: 0.0013\n",
      "Epoch 86/100\n",
      "169/169 [==============================] - 0s 947us/step - loss: 0.0088 - val_loss: 5.1915e-04\n",
      "Epoch 87/100\n",
      "169/169 [==============================] - 0s 962us/step - loss: 0.0046 - val_loss: 7.7691e-04\n",
      "Epoch 88/100\n",
      "169/169 [==============================] - 0s 953us/step - loss: 0.0081 - val_loss: 7.2722e-04\n",
      "Epoch 89/100\n",
      "169/169 [==============================] - 0s 975us/step - loss: 0.0066 - val_loss: 0.0023\n",
      "Epoch 90/100\n",
      "169/169 [==============================] - 0s 970us/step - loss: 0.0112 - val_loss: 0.0013\n",
      "Epoch 91/100\n",
      "169/169 [==============================] - 0s 967us/step - loss: 0.0073 - val_loss: 7.6065e-04\n",
      "Epoch 92/100\n",
      "169/169 [==============================] - 0s 959us/step - loss: 0.0117 - val_loss: 4.5160e-04\n",
      "Epoch 93/100\n",
      "169/169 [==============================] - 0s 971us/step - loss: 0.0055 - val_loss: 6.6290e-04\n",
      "Epoch 94/100\n",
      "169/169 [==============================] - 0s 970us/step - loss: 0.0065 - val_loss: 5.7930e-04\n",
      "Epoch 95/100\n",
      "169/169 [==============================] - 0s 952us/step - loss: 0.0040 - val_loss: 0.0018\n",
      "Epoch 96/100\n",
      "169/169 [==============================] - 0s 964us/step - loss: 0.0087 - val_loss: 0.0018\n",
      "Epoch 97/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0062 - val_loss: 0.0013\n",
      "Epoch 98/100\n",
      "169/169 [==============================] - 0s 965us/step - loss: 0.0108 - val_loss: 6.8144e-04\n",
      "Epoch 99/100\n",
      "169/169 [==============================] - 0s 958us/step - loss: 0.0076 - val_loss: 0.0036\n",
      "Epoch 100/100\n",
      "169/169 [==============================] - 0s 970us/step - loss: 0.0094 - val_loss: 0.0019\n",
      "211/211 [==============================] - 0s 557us/step\n",
      "Training RNN model...\n",
      "169/169 [==============================] - 0s 768us/step\n",
      "43/43 [==============================] - 0s 833us/step\n",
      "Training LSTM model...\n",
      "169/169 [==============================] - 0s 875us/step\n",
      "43/43 [==============================] - 0s 952us/step\n",
      "Training GRU model...\n",
      "169/169 [==============================] - 0s 952us/step\n",
      "43/43 [==============================] - 0s 976us/step\n",
      "RNN model:\n",
      " MSE (Train): 153713.58\n",
      " MSE (Test): 156506.25\n",
      " R2 (Train): 0.49\n",
      " R2 (Test): 0.46\n",
      "LSTM model:\n",
      " MSE (Train): 142864.20\n",
      " MSE (Test): 147012.88\n",
      " R2 (Train): 0.52\n",
      " R2 (Test): 0.49\n",
      "GRU model:\n",
      " MSE (Train): 176788.74\n",
      " MSE (Test): 167778.84\n",
      " R2 (Train): 0.41\n",
      " R2 (Test): 0.42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, SimpleRNN, LSTM, GRU\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"swiss_no_outliers_and_cleaned.csv\")\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[[\"Rooms\", \"Footage\", \"Distance_to_City_Center(km)\", \"Address_Latitude\", \"Address_Longitude\"]]\n",
    "y = df[\"Rent\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Define the input layer\n",
    "input_data = Input(shape=(5,))\n",
    "\n",
    "# Define the encoder\n",
    "encoded = Dense(32, activation='relu')(input_data)\n",
    "encoded = Dense(16, activation='relu')(encoded)\n",
    "\n",
    "# Define the bottleneck (compressed representation)\n",
    "bottleneck = Dense(8, activation='relu')(encoded)\n",
    "\n",
    "# Define the decoder\n",
    "decoded = Dense(16, activation='relu')(bottleneck)\n",
    "decoded = Dense(32, activation='relu')(decoded)\n",
    "\n",
    "# Define the output layer\n",
    "output_data = Dense(5, activation='linear')(decoded)\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = Model(input_data, output_data)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder model\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Extract the encoder part of the autoencoder\n",
    "encoder = Model(input_data, bottleneck)\n",
    "\n",
    "# Generate a lower-dimensional representation of your input data\n",
    "X_encoded = encoder.predict(X_scaled)\n",
    "\n",
    "# Reshape the encoded data into a suitable format for RNNs, LSTMs, or GRUs\n",
    "timesteps = 2\n",
    "features = 4\n",
    "X_encoded_reshaped = X_encoded.reshape((-1, timesteps, features))\n",
    "\n",
    "# Split the reshaped data into training and testing sets\n",
    "X_encoded_train, X_encoded_test, y_train, y_test = train_test_split(X_encoded_reshaped, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a function to create a model based on the model type\n",
    "def create_model(model_type, units):\n",
    "    model = Sequential()\n",
    "\n",
    "    if model_type == \"RNN\":\n",
    "        model.add(SimpleRNN(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(SimpleRNN(units, activation='relu', return_sequences=True))\n",
    "        model.add(SimpleRNN(units, activation='relu'))\n",
    "    elif model_type == \"LSTM\":\n",
    "        model.add(LSTM(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(LSTM(units, activation='relu', return_sequences=True))\n",
    "        model.add(LSTM(units, activation='relu'))\n",
    "    elif model_type == \"GRU\":\n",
    "        model.add(GRU(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(GRU(units, activation='relu', return_sequences=True))\n",
    "        model.add(GRU(units, activation='relu'))\n",
    "\n",
    "    model.add(Dense(units, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train the models and compare their performance\n",
    "model_types = [\"RNN\", \"LSTM\", \"GRU\"]\n",
    "units = 16\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"Training {model_type} model...\")\n",
    "    model = create_model(model_type, units)\n",
    "    model.fit(X_encoded_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "    y_pred_train = model.predict(X_encoded_train)\n",
    "    y_pred_test = model.predict(X_encoded_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    results[model_type] = {\n",
    "        \"mse_train\": mse_train,\n",
    "        \"mse_test\": mse_test,\n",
    "        \"r2_train\": r2_train,\n",
    "        \"r2_test\": r2_test,\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "for model_type, result in results.items():\n",
    "    print(f\"{model_type} model:\")\n",
    "    print(f\" MSE (Train): {result['mse_train']:.2f}\")\n",
    "    print(f\" MSE (Test): {result['mse_test']:.2f}\")\n",
    "    print(f\" R2 (Train): {result['r2_train']:.2f}\")\n",
    "    print(f\" R2 (Test): {result['r2_test']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "169/169 [==============================] - 1s 2ms/step - loss: 0.6185 - val_loss: 0.2130\n",
      "Epoch 2/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.2248 - val_loss: 0.1788\n",
      "Epoch 3/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1236 - val_loss: 0.0478\n",
      "Epoch 4/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1044 - val_loss: 0.0485\n",
      "Epoch 5/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1413 - val_loss: 0.0447\n",
      "Epoch 6/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0542 - val_loss: 0.0676\n",
      "Epoch 7/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0389\n",
      "Epoch 8/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0751 - val_loss: 0.0457\n",
      "Epoch 9/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0479 - val_loss: 0.0771\n",
      "Epoch 10/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1037 - val_loss: 0.0361\n",
      "Epoch 11/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0485 - val_loss: 0.0338\n",
      "Epoch 12/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0614 - val_loss: 0.0368\n",
      "Epoch 13/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0345\n",
      "Epoch 14/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0903 - val_loss: 0.0353\n",
      "Epoch 15/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0344 - val_loss: 0.0326\n",
      "Epoch 16/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0305 - val_loss: 0.0551\n",
      "Epoch 17/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0534 - val_loss: 0.0259\n",
      "Epoch 18/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0291 - val_loss: 0.0168\n",
      "Epoch 19/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0254 - val_loss: 0.0107\n",
      "Epoch 20/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0298 - val_loss: 0.0085\n",
      "Epoch 21/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0601 - val_loss: 0.0633\n",
      "Epoch 22/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0311 - val_loss: 0.0106\n",
      "Epoch 23/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0459 - val_loss: 0.0242\n",
      "Epoch 24/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0344 - val_loss: 0.0125\n",
      "Epoch 25/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0706 - val_loss: 0.0041\n",
      "Epoch 26/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0092 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0085 - val_loss: 0.0160\n",
      "Epoch 28/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0259 - val_loss: 0.0028\n",
      "Epoch 29/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0514 - val_loss: 0.0075\n",
      "Epoch 30/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0030\n",
      "Epoch 31/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0308 - val_loss: 0.0156\n",
      "Epoch 32/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0404 - val_loss: 0.0035\n",
      "Epoch 33/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0312 - val_loss: 0.0021\n",
      "Epoch 34/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0145 - val_loss: 0.0130\n",
      "Epoch 35/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0377 - val_loss: 0.0412\n",
      "Epoch 36/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0289 - val_loss: 0.0023\n",
      "Epoch 37/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0552 - val_loss: 0.0182\n",
      "Epoch 38/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0092 - val_loss: 0.0039\n",
      "Epoch 39/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0036 - val_loss: 0.0015\n",
      "Epoch 40/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0031 - val_loss: 0.0012\n",
      "Epoch 41/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0050 - val_loss: 0.0014\n",
      "Epoch 42/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0142 - val_loss: 0.0061\n",
      "Epoch 43/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0245 - val_loss: 0.0294\n",
      "Epoch 44/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0681 - val_loss: 0.0038\n",
      "Epoch 45/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0111 - val_loss: 0.0069\n",
      "Epoch 46/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0220 - val_loss: 0.0026\n",
      "Epoch 47/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0024\n",
      "Epoch 48/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0201 - val_loss: 0.0033\n",
      "Epoch 49/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0659 - val_loss: 0.0070\n",
      "Epoch 50/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0092 - val_loss: 0.0055\n",
      "Epoch 51/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0171 - val_loss: 0.0024\n",
      "Epoch 52/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0222 - val_loss: 0.0047\n",
      "Epoch 53/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0592 - val_loss: 0.0039\n",
      "Epoch 54/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0039 - val_loss: 0.0015\n",
      "Epoch 55/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0030 - val_loss: 0.0015\n",
      "Epoch 56/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0036 - val_loss: 0.0013\n",
      "Epoch 57/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0066 - val_loss: 0.0014\n",
      "Epoch 58/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0192 - val_loss: 0.0126\n",
      "Epoch 59/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0572 - val_loss: 0.0455\n",
      "Epoch 60/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1070 - val_loss: 0.0263\n",
      "Epoch 61/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0035\n",
      "Epoch 62/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0093 - val_loss: 0.0017\n",
      "Epoch 63/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0189 - val_loss: 0.0016\n",
      "Epoch 64/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0105 - val_loss: 0.0126\n",
      "Epoch 65/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0342 - val_loss: 0.0012\n",
      "Epoch 66/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0057 - val_loss: 0.0089\n",
      "Epoch 67/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0220 - val_loss: 0.0012\n",
      "Epoch 68/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0079 - val_loss: 0.0017\n",
      "Epoch 69/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0170 - val_loss: 0.0016\n",
      "Epoch 70/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0018\n",
      "Epoch 71/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0330 - val_loss: 0.0110\n",
      "Epoch 72/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0065 - val_loss: 0.0012\n",
      "Epoch 73/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0099 - val_loss: 0.0019\n",
      "Epoch 74/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0104 - val_loss: 0.0017\n",
      "Epoch 75/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0245 - val_loss: 0.0029\n",
      "Epoch 76/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0110 - val_loss: 0.0010\n",
      "Epoch 77/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0281 - val_loss: 0.0023\n",
      "Epoch 78/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0086 - val_loss: 0.0021\n",
      "Epoch 79/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0190 - val_loss: 0.0020\n",
      "Epoch 80/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0013\n",
      "Epoch 81/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0273 - val_loss: 0.0082\n",
      "Epoch 82/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0094 - val_loss: 0.0028\n",
      "Epoch 83/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0236 - val_loss: 0.0011\n",
      "Epoch 84/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0026\n",
      "Epoch 85/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0246 - val_loss: 0.0017\n",
      "Epoch 86/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0064 - val_loss: 0.0016\n",
      "Epoch 87/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0012\n",
      "Epoch 88/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0030\n",
      "Epoch 89/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0262 - val_loss: 0.0057\n",
      "Epoch 90/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0095 - val_loss: 0.0042\n",
      "Epoch 91/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0025\n",
      "Epoch 92/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0110 - val_loss: 0.0020\n",
      "Epoch 93/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0240 - val_loss: 0.0036\n",
      "Epoch 94/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0070 - val_loss: 0.0016\n",
      "Epoch 95/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0110 - val_loss: 0.0030\n",
      "Epoch 96/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0036\n",
      "Epoch 97/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0453 - val_loss: 0.0053\n",
      "Epoch 98/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0272 - val_loss: 0.0109\n",
      "Epoch 99/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0248 - val_loss: 0.0154\n",
      "Epoch 100/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0509 - val_loss: 0.0022\n",
      "Epoch 101/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0533 - val_loss: 0.0037\n",
      "Epoch 102/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0073 - val_loss: 0.0011\n",
      "Epoch 103/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 104/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0068 - val_loss: 0.0012\n",
      "Epoch 105/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0094 - val_loss: 0.0014\n",
      "Epoch 106/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0241 - val_loss: 0.0061\n",
      "Epoch 107/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0051 - val_loss: 0.0017\n",
      "Epoch 108/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0080 - val_loss: 0.0013\n",
      "Epoch 109/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0064 - val_loss: 9.6663e-04\n",
      "Epoch 110/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0011\n",
      "Epoch 111/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0032\n",
      "Epoch 112/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0268 - val_loss: 0.0037\n",
      "Epoch 113/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0106 - val_loss: 0.0061\n",
      "Epoch 114/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0320 - val_loss: 0.0019\n",
      "Epoch 115/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0010\n",
      "Epoch 116/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0190 - val_loss: 0.0125\n",
      "Epoch 117/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0093 - val_loss: 0.0015\n",
      "Epoch 118/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0192 - val_loss: 7.8424e-04\n",
      "Epoch 119/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0103 - val_loss: 0.0013\n",
      "Epoch 120/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0025\n",
      "Epoch 121/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0074 - val_loss: 0.0012\n",
      "Epoch 122/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 7.0115e-04\n",
      "Epoch 123/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0084 - val_loss: 0.0134\n",
      "Epoch 124/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0252 - val_loss: 0.0019\n",
      "Epoch 125/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0073 - val_loss: 0.0022\n",
      "Epoch 126/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0192 - val_loss: 0.0023\n",
      "Epoch 127/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0096 - val_loss: 0.0069\n",
      "Epoch 128/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0239 - val_loss: 0.0012\n",
      "Epoch 129/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0070 - val_loss: 0.0015\n",
      "Epoch 130/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 8.8692e-04\n",
      "Epoch 131/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0080 - val_loss: 0.0025\n",
      "Epoch 132/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0183 - val_loss: 0.0028\n",
      "Epoch 133/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0095 - val_loss: 0.0067\n",
      "Epoch 134/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 8.5691e-04\n",
      "Epoch 135/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0112\n",
      "Epoch 136/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0254 - val_loss: 0.0060\n",
      "Epoch 137/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0031\n",
      "Epoch 138/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0196 - val_loss: 0.0029\n",
      "Epoch 139/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0075 - val_loss: 0.0068\n",
      "Epoch 140/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0147 - val_loss: 0.0019\n",
      "Epoch 141/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0086 - val_loss: 0.0023\n",
      "Epoch 142/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0151 - val_loss: 0.0053\n",
      "Epoch 143/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0107 - val_loss: 0.0042\n",
      "Epoch 144/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0162 - val_loss: 0.0025\n",
      "Epoch 145/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0057 - val_loss: 0.0063\n",
      "Epoch 146/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 8.7237e-04\n",
      "Epoch 147/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0191 - val_loss: 0.0596\n",
      "Epoch 148/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0361 - val_loss: 0.0011\n",
      "Epoch 149/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0109 - val_loss: 0.0071\n",
      "Epoch 150/150\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0246 - val_loss: 0.0013\n",
      "211/211 [==============================] - 0s 586us/step\n",
      "Training RNN model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining \u001b[39m\u001b[39m{\u001b[39;00mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m model...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     98\u001b[0m model \u001b[39m=\u001b[39m create_model(model_type, units)\n\u001b[1;32m---> 99\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_encoded_train, y_train, epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m    101\u001b[0m y_pred_train \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_encoded_train)\n\u001b[0;32m    102\u001b[0m y_pred_test \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_encoded_test)\n",
      "File \u001b[1;32mc:\\Users\\remy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\remy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\remy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\remy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\remy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\remy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\remy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\remy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\remy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, SimpleRNN, LSTM, GRU, Dropout\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"swiss_no_outliers_and_cleaned.csv\")\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[[\"Rooms\", \"Footage\", \"Distance_to_City_Center(km)\", \"Address_Latitude\", \"Address_Longitude\"]]\n",
    "y = df[\"Rent\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Define the input layer\n",
    "input_data = Input(shape=(5,))\n",
    "\n",
    "# Define the encoder\n",
    "encoded = Dense(64, activation='relu')(input_data)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "encoded = Dense(16, activation='relu')(encoded)\n",
    "\n",
    "# Define the bottleneck (compressed representation)\n",
    "bottleneck = Dense(8, activation='relu')(encoded)\n",
    "\n",
    "# Define the decoder\n",
    "decoded = Dense(16, activation='relu')(bottleneck)\n",
    "decoded = Dense(32, activation='relu')(decoded)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "\n",
    "# Define the output layer\n",
    "output_data = Dense(5, activation='linear')(decoded)\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = Model(input_data, output_data)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder model\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=150, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Extract the encoder part of the autoencoder\n",
    "encoder = Model(input_data, bottleneck)\n",
    "\n",
    "# Generate a lower-dimensional representation of your input data\n",
    "X_encoded = encoder.predict(X_scaled)\n",
    "\n",
    "# Reshape the encoded data into a suitable format for RNNs, LSTMs, or GRUs\n",
    "timesteps = 4\n",
    "features = 2\n",
    "X_encoded_reshaped = X_encoded.reshape((-1, timesteps, features))\n",
    "\n",
    "# Split the reshaped data into training and testing sets\n",
    "X_encoded_train, X_encoded_test, y_train, y_test = train_test_split(X_encoded_reshaped, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "def create_model(model_type, units):\n",
    "    model = Sequential()\n",
    "\n",
    "    if model_type == \"RNN\":\n",
    "        model.add(SimpleRNN(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(SimpleRNN(units, activation='relu'))\n",
    "    elif model_type == \"LSTM\":\n",
    "        model.add(LSTM(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(units, activation='relu'))\n",
    "    elif model_type == \"GRU\":\n",
    "        model.add(GRU(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(GRU(units, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Train the models and compare their performance\n",
    "model_types = [\"RNN\", \"LSTM\", \"GRU\"]\n",
    "units = 32\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "# The rest of the code remains unchanged\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"Training {model_type} model...\")\n",
    "    model = create_model(model_type, units)\n",
    "    model.fit(X_encoded_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "    y_pred_train = model.predict(X_encoded_train)\n",
    "    y_pred_test = model.predict(X_encoded_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    results[model_type] = {\n",
    "        \"mse_train\": mse_train,\n",
    "        \"mse_test\": mse_test,\n",
    "        \"r2_train\": r2_train,\n",
    "        \"r2_test\": r2_test,\n",
    "    }\n",
    "    # Assuming your model is named 'model'\n",
    "    model.save(f\"{model_type}_model.h5\")\n",
    "\n",
    "\n",
    "# Print the results\n",
    "for model_type, result in results.items():\n",
    "    print(f\"{model_type} model:\")\n",
    "    print(f\" MSE (Train): {result['mse_train']:.2f}\")\n",
    "    print(f\" MSE (Test): {result['mse_test']:.2f}\")\n",
    "    print(f\" R2 (Train): {result['r2_train']:.2f}\")\n",
    "    print(f\" R2 (Test): {result['r2_test']:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "169/169 [==============================] - 1s 1ms/step - loss: 0.4048 - val_loss: 0.0826\n",
      "Epoch 2/100\n",
      "169/169 [==============================] - 0s 971us/step - loss: 0.0642 - val_loss: 0.0704\n",
      "Epoch 3/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.1179\n",
      "Epoch 4/100\n",
      "169/169 [==============================] - 0s 977us/step - loss: 0.1787 - val_loss: 0.0253\n",
      "Epoch 5/100\n",
      "169/169 [==============================] - 0s 976us/step - loss: 0.0223 - val_loss: 0.0054\n",
      "Epoch 6/100\n",
      "169/169 [==============================] - 0s 988us/step - loss: 0.0251 - val_loss: 0.0266\n",
      "Epoch 7/100\n",
      "169/169 [==============================] - 0s 997us/step - loss: 0.0659 - val_loss: 0.0049\n",
      "Epoch 8/100\n",
      "169/169 [==============================] - 0s 997us/step - loss: 0.0097 - val_loss: 0.0054\n",
      "Epoch 9/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0228 - val_loss: 0.0035\n",
      "Epoch 10/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0233 - val_loss: 0.0085\n",
      "Epoch 11/100\n",
      "169/169 [==============================] - 0s 973us/step - loss: 0.0565 - val_loss: 0.0053\n",
      "Epoch 12/100\n",
      "169/169 [==============================] - 0s 970us/step - loss: 0.0092 - val_loss: 0.0053\n",
      "Epoch 13/100\n",
      "169/169 [==============================] - 0s 988us/step - loss: 0.0133 - val_loss: 0.0027\n",
      "Epoch 14/100\n",
      "169/169 [==============================] - 0s 985us/step - loss: 0.0408 - val_loss: 0.0129\n",
      "Epoch 15/100\n",
      "169/169 [==============================] - 0s 982us/step - loss: 0.0815 - val_loss: 0.0028\n",
      "Epoch 16/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0144 - val_loss: 0.0045\n",
      "Epoch 17/100\n",
      "169/169 [==============================] - 0s 982us/step - loss: 0.0139 - val_loss: 0.0157\n",
      "Epoch 18/100\n",
      "169/169 [==============================] - 0s 982us/step - loss: 0.0255 - val_loss: 0.0175\n",
      "Epoch 19/100\n",
      "169/169 [==============================] - 0s 994us/step - loss: 0.0202 - val_loss: 0.0236\n",
      "Epoch 20/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0632 - val_loss: 0.0037\n",
      "Epoch 21/100\n",
      "169/169 [==============================] - 0s 997us/step - loss: 0.0329 - val_loss: 0.0067\n",
      "Epoch 22/100\n",
      "169/169 [==============================] - 0s 976us/step - loss: 0.0464 - val_loss: 0.0084\n",
      "Epoch 23/100\n",
      "169/169 [==============================] - 0s 973us/step - loss: 0.0110 - val_loss: 0.0068\n",
      "Epoch 24/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0182 - val_loss: 0.0028\n",
      "Epoch 25/100\n",
      "169/169 [==============================] - 0s 994us/step - loss: 0.0175 - val_loss: 0.0028\n",
      "Epoch 26/100\n",
      "169/169 [==============================] - 0s 979us/step - loss: 0.0348 - val_loss: 0.0026\n",
      "Epoch 27/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0103 - val_loss: 0.0075\n",
      "Epoch 28/100\n",
      "169/169 [==============================] - 0s 988us/step - loss: 0.0173 - val_loss: 0.0040\n",
      "Epoch 29/100\n",
      "169/169 [==============================] - 0s 982us/step - loss: 0.0184 - val_loss: 0.0023\n",
      "Epoch 30/100\n",
      "169/169 [==============================] - 0s 994us/step - loss: 0.0366 - val_loss: 0.0059\n",
      "Epoch 31/100\n",
      "169/169 [==============================] - 0s 976us/step - loss: 0.0073 - val_loss: 0.0125\n",
      "Epoch 32/100\n",
      "169/169 [==============================] - 0s 970us/step - loss: 0.0209 - val_loss: 0.0169\n",
      "Epoch 33/100\n",
      "169/169 [==============================] - 0s 988us/step - loss: 0.0345 - val_loss: 0.0078\n",
      "Epoch 34/100\n",
      "169/169 [==============================] - 0s 987us/step - loss: 0.0367 - val_loss: 0.0245\n",
      "Epoch 35/100\n",
      "169/169 [==============================] - 0s 982us/step - loss: 0.0246 - val_loss: 0.0036\n",
      "Epoch 36/100\n",
      "169/169 [==============================] - 0s 988us/step - loss: 0.0133 - val_loss: 0.0027\n",
      "Epoch 37/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0024\n",
      "Epoch 38/100\n",
      "169/169 [==============================] - 0s 976us/step - loss: 0.0313 - val_loss: 0.0015\n",
      "Epoch 39/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0055 - val_loss: 0.0016\n",
      "Epoch 40/100\n",
      "169/169 [==============================] - 0s 982us/step - loss: 0.0114 - val_loss: 0.0013\n",
      "Epoch 41/100\n",
      "169/169 [==============================] - 0s 983us/step - loss: 0.0119 - val_loss: 0.0014\n",
      "Epoch 42/100\n",
      "169/169 [==============================] - 0s 983us/step - loss: 0.0229 - val_loss: 0.0022\n",
      "Epoch 43/100\n",
      "169/169 [==============================] - 0s 984us/step - loss: 0.0169 - val_loss: 0.0017\n",
      "Epoch 44/100\n",
      "169/169 [==============================] - 0s 994us/step - loss: 0.0376 - val_loss: 0.0133\n",
      "Epoch 45/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0019\n",
      "Epoch 46/100\n",
      "169/169 [==============================] - 0s 988us/step - loss: 0.0272 - val_loss: 0.0013\n",
      "Epoch 47/100\n",
      "169/169 [==============================] - 0s 989us/step - loss: 0.0107 - val_loss: 0.0014\n",
      "Epoch 48/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0245 - val_loss: 0.0095\n",
      "Epoch 49/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0191 - val_loss: 0.0023\n",
      "Epoch 50/100\n",
      "169/169 [==============================] - 0s 989us/step - loss: 0.0287 - val_loss: 0.0012\n",
      "Epoch 51/100\n",
      "169/169 [==============================] - 0s 997us/step - loss: 0.0050 - val_loss: 0.0017\n",
      "Epoch 52/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0214 - val_loss: 0.0073\n",
      "Epoch 53/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0277 - val_loss: 0.0123\n",
      "Epoch 54/100\n",
      "169/169 [==============================] - 0s 994us/step - loss: 0.0520 - val_loss: 0.0016\n",
      "Epoch 55/100\n",
      "169/169 [==============================] - 0s 970us/step - loss: 0.0031 - val_loss: 0.0018\n",
      "Epoch 56/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0027 - val_loss: 0.0023\n",
      "Epoch 57/100\n",
      "169/169 [==============================] - 0s 996us/step - loss: 0.0061 - val_loss: 0.0045\n",
      "Epoch 58/100\n",
      "169/169 [==============================] - 0s 983us/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 59/100\n",
      "169/169 [==============================] - 0s 968us/step - loss: 0.0243 - val_loss: 9.8527e-04\n",
      "Epoch 60/100\n",
      "169/169 [==============================] - 0s 982us/step - loss: 0.0081 - val_loss: 0.0023\n",
      "Epoch 61/100\n",
      "169/169 [==============================] - 0s 982us/step - loss: 0.0198 - val_loss: 0.0012\n",
      "Epoch 62/100\n",
      "169/169 [==============================] - 0s 969us/step - loss: 0.0137 - val_loss: 0.0064\n",
      "Epoch 63/100\n",
      "169/169 [==============================] - 0s 982us/step - loss: 0.0308 - val_loss: 0.0123\n",
      "Epoch 64/100\n",
      "169/169 [==============================] - 0s 988us/step - loss: 0.0074 - val_loss: 0.0022\n",
      "Epoch 65/100\n",
      "169/169 [==============================] - 0s 976us/step - loss: 0.0136 - val_loss: 0.0037\n",
      "Epoch 66/100\n",
      "169/169 [==============================] - 0s 994us/step - loss: 0.0128 - val_loss: 0.0033\n",
      "Epoch 67/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0235 - val_loss: 0.0102\n",
      "Epoch 68/100\n",
      "169/169 [==============================] - 0s 993us/step - loss: 0.0086 - val_loss: 0.0020\n",
      "Epoch 69/100\n",
      "169/169 [==============================] - 0s 995us/step - loss: 0.0168 - val_loss: 0.0087\n",
      "Epoch 70/100\n",
      "169/169 [==============================] - 0s 965us/step - loss: 0.0149 - val_loss: 0.0101\n",
      "Epoch 71/100\n",
      "169/169 [==============================] - 0s 993us/step - loss: 0.0290 - val_loss: 0.0012\n",
      "Epoch 72/100\n",
      "169/169 [==============================] - 0s 994us/step - loss: 0.0074 - val_loss: 0.0037\n",
      "Epoch 73/100\n",
      "169/169 [==============================] - 0s 977us/step - loss: 0.0166 - val_loss: 0.0078\n",
      "Epoch 74/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0031 - val_loss: 0.0020\n",
      "Epoch 75/100\n",
      "169/169 [==============================] - 0s 1000us/step - loss: 0.0060 - val_loss: 0.0015\n",
      "Epoch 76/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 9.5678e-04\n",
      "Epoch 77/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0036\n",
      "Epoch 78/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0330 - val_loss: 9.4191e-04\n",
      "Epoch 79/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0034 - val_loss: 9.2420e-04\n",
      "Epoch 80/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0088 - val_loss: 0.0044\n",
      "Epoch 81/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0068\n",
      "Epoch 82/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0306 - val_loss: 0.0014\n",
      "Epoch 83/100\n",
      "169/169 [==============================] - 0s 982us/step - loss: 0.0041 - val_loss: 0.0016\n",
      "Epoch 84/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0088 - val_loss: 0.0035\n",
      "Epoch 85/100\n",
      "169/169 [==============================] - 0s 988us/step - loss: 0.0129 - val_loss: 0.0055\n",
      "Epoch 86/100\n",
      "169/169 [==============================] - 0s 988us/step - loss: 0.0385 - val_loss: 0.0016\n",
      "Epoch 87/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0025 - val_loss: 8.2596e-04\n",
      "Epoch 88/100\n",
      "169/169 [==============================] - 0s 994us/step - loss: 0.0037 - val_loss: 7.4482e-04\n",
      "Epoch 89/100\n",
      "169/169 [==============================] - 0s 977us/step - loss: 0.0056 - val_loss: 9.8026e-04\n",
      "Epoch 90/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0014\n",
      "Epoch 91/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0097 - val_loss: 0.0012\n",
      "Epoch 92/100\n",
      "169/169 [==============================] - 0s 993us/step - loss: 0.0223 - val_loss: 0.0070\n",
      "Epoch 93/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0189 - val_loss: 0.0020\n",
      "Epoch 94/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0364 - val_loss: 0.0018\n",
      "Epoch 95/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 96/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0026 - val_loss: 8.6397e-04\n",
      "Epoch 97/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0034 - val_loss: 0.0036\n",
      "Epoch 98/100\n",
      "169/169 [==============================] - 0s 994us/step - loss: 0.0081 - val_loss: 0.0047\n",
      "Epoch 99/100\n",
      "169/169 [==============================] - 0s 976us/step - loss: 0.0190 - val_loss: 0.0128\n",
      "Epoch 100/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0320 - val_loss: 0.0012\n",
      "211/211 [==============================] - 0s 576us/step\n",
      "Training RNN model...\n",
      "169/169 [==============================] - 0s 690us/step\n",
      "43/43 [==============================] - 0s 714us/step\n",
      "Training LSTM model...\n",
      "169/169 [==============================] - 0s 1ms/step\n",
      "43/43 [==============================] - 0s 1ms/step\n",
      "Training GRU model...\n",
      "169/169 [==============================] - 0s 946us/step\n",
      "43/43 [==============================] - 0s 905us/step\n",
      "RNN model:\n",
      " MSE (Train): 178563.14\n",
      " MSE (Test): 169461.42\n",
      " R2 (Train): 0.40\n",
      " R2 (Test): 0.42\n",
      "LSTM model:\n",
      " MSE (Train): 138772.61\n",
      " MSE (Test): 144383.26\n",
      " R2 (Train): 0.54\n",
      " R2 (Test): 0.50\n",
      "GRU model:\n",
      " MSE (Train): 150236.84\n",
      " MSE (Test): 143825.75\n",
      " R2 (Train): 0.50\n",
      " R2 (Test): 0.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, SimpleRNN, LSTM, GRU, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"swiss_no_outliers_and_cleaned.csv\")\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[[\"Rooms\", \"Footage\", \"Distance_to_City_Center(km)\", \"Address_Latitude\", \"Address_Longitude\"]]\n",
    "y = df[\"Rent\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Define the input layer\n",
    "input_data = Input(shape=(5,))\n",
    "\n",
    "# Define the encoder\n",
    "encoded = Dense(64, activation='relu')(input_data)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "# Define the bottleneck (compressed representation)\n",
    "bottleneck = Dense(8, activation='relu')(encoded)\n",
    "\n",
    "# Define the decoder\n",
    "decoded = Dense(32, activation='relu')(bottleneck)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "\n",
    "# Define the output layer\n",
    "output_data = Dense(5, activation='linear')(decoded)\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = Model(input_data, output_data)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder model\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Extract the encoder part of the autoencoder\n",
    "encoder = Model(input_data, bottleneck)\n",
    "\n",
    "# Generate a lower-dimensional representation of your input data\n",
    "X_encoded = encoder.predict(X_scaled)\n",
    "\n",
    "# Reshape the encoded data into a suitable format for RNNs, LSTMs, or GRUs\n",
    "timesteps = 2\n",
    "features = 4\n",
    "X_encoded_reshaped = X_encoded.reshape((-1, timesteps, features))\n",
    "\n",
    "# Split the reshaped data into training and testing sets\n",
    "X_encoded_train, X_encoded_test, y_train, y_test = train_test_split(X_encoded_reshaped, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a function to create a model based on the model type\n",
    "def create_model(model_type, units):\n",
    "    model = Sequential()\n",
    "\n",
    "    if model_type == \"RNN\":\n",
    "        model.add(SimpleRNN(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(SimpleRNN(units, activation='relu'))\n",
    "    elif model_type == \"LSTM\":\n",
    "        model.add(LSTM(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(units, activation='relu'))\n",
    "    elif model_type == \"GRU\":\n",
    "        model.add(GRU(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(GRU(units, activation='relu'))\n",
    "\n",
    "    model.add(Dense(units, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "# Train the models and compare their performance\n",
    "model_types = [\"RNN\", \"LSTM\", \"GRU\"]\n",
    "units = 32\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"Training {model_type} model...\")\n",
    "    model = create_model(model_type, units)\n",
    "    model.fit(X_encoded_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "    y_pred_train = model.predict(X_encoded_train)\n",
    "    y_pred_test = model.predict(X_encoded_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    results[model_type] = {\n",
    "        \"mse_train\": mse_train,\n",
    "        \"mse_test\": mse_test,\n",
    "        \"r2_train\": r2_train,\n",
    "        \"r2_test\": r2_test,\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "for model_type, result in results.items():\n",
    "    print(f\"{model_type} model:\")\n",
    "    print(f\" MSE (Train): {result['mse_train']:.2f}\")\n",
    "    print(f\" MSE (Test): {result['mse_test']:.2f}\")\n",
    "    print(f\" R2 (Train): {result['r2_train']:.2f}\")\n",
    "    print(f\" R2 (Test): {result['r2_test']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "169/169 [==============================] - 1s 1ms/step - loss: 0.5332 - val_loss: 0.1614\n",
      "Epoch 2/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1677 - val_loss: 0.0405\n",
      "Epoch 3/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0871 - val_loss: 0.0260\n",
      "Epoch 4/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0455 - val_loss: 0.0134\n",
      "Epoch 5/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0211 - val_loss: 0.0073\n",
      "Epoch 6/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0143 - val_loss: 0.0087\n",
      "Epoch 7/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0238 - val_loss: 0.0057\n",
      "Epoch 8/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 0.0408\n",
      "Epoch 9/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0451 - val_loss: 0.0506\n",
      "Epoch 10/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0181 - val_loss: 0.0146\n",
      "Epoch 11/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0096 - val_loss: 0.0063\n",
      "Epoch 12/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0078 - val_loss: 0.0145\n",
      "Epoch 13/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0094\n",
      "Epoch 14/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0201 - val_loss: 0.0043\n",
      "Epoch 15/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0228 - val_loss: 0.0038\n",
      "Epoch 16/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 0.0160\n",
      "Epoch 17/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0150 - val_loss: 0.0040\n",
      "Epoch 18/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0109 - val_loss: 0.0031\n",
      "Epoch 19/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0045\n",
      "Epoch 20/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0102 - val_loss: 0.0068\n",
      "Epoch 21/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0170 - val_loss: 0.0146\n",
      "Epoch 22/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0104 - val_loss: 0.0046\n",
      "Epoch 23/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 0.0021\n",
      "Epoch 24/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0097 - val_loss: 0.0050\n",
      "Epoch 25/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0306\n",
      "Epoch 26/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0252 - val_loss: 0.0039\n",
      "Epoch 27/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0107\n",
      "Epoch 28/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0054 - val_loss: 0.0018\n",
      "Epoch 29/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0063 - val_loss: 0.0034\n",
      "Epoch 30/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0077 - val_loss: 0.0067\n",
      "Epoch 31/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0174 - val_loss: 0.0157\n",
      "Epoch 32/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0192 - val_loss: 0.0043\n",
      "Epoch 33/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0239 - val_loss: 0.0147\n",
      "Epoch 34/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0274 - val_loss: 0.0034\n",
      "Epoch 35/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 36/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0028 - val_loss: 0.0018\n",
      "Epoch 37/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.0016\n",
      "Epoch 38/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0063 - val_loss: 0.0014\n",
      "Epoch 39/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0105 - val_loss: 0.0071\n",
      "Epoch 40/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0143 - val_loss: 0.0017\n",
      "Epoch 41/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0219 - val_loss: 0.0021\n",
      "Epoch 42/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0054 - val_loss: 0.0014\n",
      "Epoch 43/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0060 - val_loss: 0.0023\n",
      "Epoch 44/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0011\n",
      "Epoch 45/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 0.0106\n",
      "Epoch 46/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0013\n",
      "Epoch 47/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0249 - val_loss: 0.0023\n",
      "Epoch 48/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0161\n",
      "Epoch 49/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0208 - val_loss: 0.0057\n",
      "Epoch 50/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0144\n",
      "Epoch 51/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0177 - val_loss: 0.0042\n",
      "Epoch 52/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0021\n",
      "Epoch 53/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0043 - val_loss: 0.0017\n",
      "Epoch 54/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0049 - val_loss: 0.0068\n",
      "Epoch 55/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0090 - val_loss: 0.0183\n",
      "Epoch 56/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0109 - val_loss: 0.0040\n",
      "Epoch 57/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 0.0011\n",
      "Epoch 58/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0075 - val_loss: 0.0022\n",
      "Epoch 59/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0103 - val_loss: 0.0033\n",
      "Epoch 60/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0145 - val_loss: 0.0017\n",
      "Epoch 61/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0192 - val_loss: 0.0025\n",
      "Epoch 62/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0036 - val_loss: 0.0020\n",
      "Epoch 63/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0053 - val_loss: 0.0013\n",
      "Epoch 64/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0013\n",
      "Epoch 65/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 0.0059\n",
      "Epoch 66/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 0.0061\n",
      "Epoch 67/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0086 - val_loss: 9.5381e-04\n",
      "Epoch 68/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0065 - val_loss: 0.0044\n",
      "Epoch 69/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0102 - val_loss: 0.0166\n",
      "Epoch 70/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0096 - val_loss: 0.0024\n",
      "Epoch 71/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0038\n",
      "Epoch 72/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0063 - val_loss: 0.0225\n",
      "Epoch 73/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0012\n",
      "Epoch 74/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0056 - val_loss: 0.0023\n",
      "Epoch 75/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 76/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0158\n",
      "Epoch 77/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0165 - val_loss: 0.0032\n",
      "Epoch 78/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0053 - val_loss: 0.0014\n",
      "Epoch 79/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0052 - val_loss: 0.0050\n",
      "Epoch 80/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 81/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0080 - val_loss: 0.0075\n",
      "Epoch 82/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0106 - val_loss: 0.0070\n",
      "Epoch 83/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0159 - val_loss: 0.0129\n",
      "Epoch 84/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0103 - val_loss: 0.0020\n",
      "Epoch 85/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0207 - val_loss: 0.0092\n",
      "Epoch 86/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0084\n",
      "Epoch 87/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0079 - val_loss: 0.0030\n",
      "Epoch 88/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0049 - val_loss: 0.0055\n",
      "Epoch 89/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0052 - val_loss: 0.0043\n",
      "Epoch 90/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0057 - val_loss: 0.0077\n",
      "Epoch 91/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 0.0011\n",
      "Epoch 92/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0046 - val_loss: 0.0028\n",
      "Epoch 93/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0070 - val_loss: 8.8709e-04\n",
      "Epoch 94/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0081 - val_loss: 0.0022\n",
      "Epoch 95/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0149 - val_loss: 0.0012\n",
      "Epoch 96/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0049 - val_loss: 0.0035\n",
      "Epoch 97/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0091 - val_loss: 0.0018\n",
      "Epoch 98/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0097 - val_loss: 0.0150\n",
      "Epoch 99/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0232 - val_loss: 0.0014\n",
      "Epoch 100/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0090 - val_loss: 0.0024\n",
      "211/211 [==============================] - 0s 576us/step\n",
      "Training RNN model...\n",
      "169/169 [==============================] - 0s 765us/step\n",
      "43/43 [==============================] - 0s 786us/step\n",
      "Training LSTM model...\n",
      "169/169 [==============================] - 0s 1ms/step\n",
      "43/43 [==============================] - 0s 1ms/step\n",
      "Training GRU model...\n",
      "169/169 [==============================] - 0s 1ms/step\n",
      "43/43 [==============================] - 0s 1ms/step\n",
      "RNN model:\n",
      " MSE (Train): 123124.67\n",
      " MSE (Test): 130525.49\n",
      " R2 (Train): 0.59\n",
      " R2 (Test): 0.55\n",
      "LSTM model:\n",
      " MSE (Train): 116012.28\n",
      " MSE (Test): 128929.47\n",
      " R2 (Train): 0.61\n",
      " R2 (Test): 0.56\n",
      "GRU model:\n",
      " MSE (Train): 113883.29\n",
      " MSE (Test): 128303.21\n",
      " R2 (Train): 0.62\n",
      " R2 (Test): 0.56\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, SimpleRNN, LSTM, GRU, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"swiss_no_outliers_and_cleaned.csv\")\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[[\"Rooms\", \"Footage\", \"Distance_to_City_Center(km)\", \"Address_Latitude\", \"Address_Longitude\"]]\n",
    "y = df[\"Rent\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Define the input layer\n",
    "input_data = Input(shape=(5,))\n",
    "\n",
    "# Define the encoder\n",
    "encoded = Dense(32, activation='relu')(input_data)\n",
    "encoded = Dense(16, activation='relu')(encoded)\n",
    "\n",
    "# Define the bottleneck (compressed representation)\n",
    "bottleneck = Dense(8, activation='relu')(encoded)\n",
    "\n",
    "# Define the decoder\n",
    "decoded = Dense(16, activation='relu')(bottleneck)\n",
    "decoded = Dense(32, activation='relu')(decoded)\n",
    "\n",
    "# Define the output layer\n",
    "output_data = Dense(5, activation='linear')(decoded)\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = Model(input_data, output_data)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder model\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Extract the encoder part of the autoencoder\n",
    "encoder = Model(input_data, bottleneck)\n",
    "\n",
    "# Generate a lower-dimensional representation of your input data\n",
    "X_encoded = encoder.predict(X_scaled)\n",
    "\n",
    "# Reshape the encoded data into a suitable format for RNNs, LSTMs, or GRUs\n",
    "timesteps = 2\n",
    "features = 4\n",
    "X_encoded_reshaped = X_encoded.reshape((-1, timesteps, features))\n",
    "\n",
    "# Split the reshaped data into training and testing sets\n",
    "X_encoded_train, X_encoded_test, y_train, y_test = train_test_split(X_encoded_reshaped, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "def create_model(model_type, units):\n",
    "    model = Sequential()\n",
    "\n",
    "    if model_type == \"RNN\":\n",
    "        model.add(SimpleRNN(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(SimpleRNN(units, activation='relu'))\n",
    "    elif model_type == \"LSTM\":\n",
    "        model.add(LSTM(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(units, activation='relu'))\n",
    "    elif model_type == \"GRU\":\n",
    "        model.add(GRU(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(GRU(units, activation='relu'))\n",
    "\n",
    "    model.add(Dense(units, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "# Train the models and compare their performance\n",
    "model_types = [\"RNN\", \"LSTM\", \"GRU\"]\n",
    "units = 64\n",
    "epochs = 300\n",
    "batch_size = 32\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"Training {model_type} model...\")\n",
    "    model = create_model(model_type, units)\n",
    "    \n",
    "    # Add early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_encoded_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    y_pred_train = model.predict(X_encoded_train)\n",
    "    y_pred_test = model.predict(X_encoded_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    results[model_type] = {\n",
    "        \"mse_train\": mse_train,\n",
    "        \"mse_test\": mse_test,\n",
    "        \"r2_train\": r2_train,\n",
    "        \"r2_test\": r2_test,\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "for model_type, result in results.items():\n",
    "    print(f\"{model_type} model:\")\n",
    "    print(f\" MSE (Train): {result['mse_train']:.2f}\")\n",
    "    print(f\" MSE (Test): {result['mse_test']:.2f}\")\n",
    "    print(f\" R2 (Train): {result['r2_train']:.2f}\")\n",
    "    print(f\" R2 (Test): {result['r2_test']:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "169/169 [==============================] - 1s 1ms/step - loss: 0.7013 - val_loss: 0.2782\n",
      "Epoch 2/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.3507 - val_loss: 0.0859\n",
      "Epoch 3/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1820 - val_loss: 0.0371\n",
      "Epoch 4/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1191 - val_loss: 0.0252\n",
      "Epoch 5/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0619 - val_loss: 0.0147\n",
      "Epoch 6/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0232 - val_loss: 0.0115\n",
      "Epoch 7/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0143 - val_loss: 0.0071\n",
      "Epoch 8/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0094 - val_loss: 0.0080\n",
      "Epoch 9/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0083 - val_loss: 0.0054\n",
      "Epoch 10/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0091 - val_loss: 0.0063\n",
      "Epoch 11/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0057\n",
      "Epoch 12/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0310 - val_loss: 0.0049\n",
      "Epoch 13/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0043\n",
      "Epoch 14/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1049 - val_loss: 0.0103\n",
      "Epoch 15/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0095 - val_loss: 0.0051\n",
      "Epoch 16/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0070 - val_loss: 0.0044\n",
      "Epoch 17/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0072 - val_loss: 0.0077\n",
      "Epoch 18/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0076 - val_loss: 0.0084\n",
      "Epoch 19/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0165 - val_loss: 0.0039\n",
      "Epoch 20/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 0.0179\n",
      "Epoch 21/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0105 - val_loss: 0.0030\n",
      "Epoch 22/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0099 - val_loss: 0.0029\n",
      "Epoch 23/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0054 - val_loss: 0.0027\n",
      "Epoch 24/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0071 - val_loss: 0.0048\n",
      "Epoch 25/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0058 - val_loss: 0.0026\n",
      "Epoch 26/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0107 - val_loss: 0.0029\n",
      "Epoch 27/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0110 - val_loss: 0.0037\n",
      "Epoch 28/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0080\n",
      "Epoch 29/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0058\n",
      "Epoch 30/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0183 - val_loss: 0.0021\n",
      "Epoch 31/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0074 - val_loss: 0.0023\n",
      "Epoch 32/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0021\n",
      "Epoch 33/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0073 - val_loss: 0.0028\n",
      "Epoch 34/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0023\n",
      "Epoch 35/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0060 - val_loss: 0.0017\n",
      "Epoch 36/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0077 - val_loss: 0.0082\n",
      "Epoch 37/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0110 - val_loss: 0.0039\n",
      "Epoch 38/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0029\n",
      "Epoch 39/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0064\n",
      "Epoch 40/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0263 - val_loss: 0.0023\n",
      "Epoch 41/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0070 - val_loss: 0.0022\n",
      "Epoch 42/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0078\n",
      "Epoch 43/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0019\n",
      "Epoch 44/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0146 - val_loss: 0.0022\n",
      "Epoch 45/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 46/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0216 - val_loss: 0.0026\n",
      "Epoch 47/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0074 - val_loss: 0.0013\n",
      "Epoch 48/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0088 - val_loss: 0.0020\n",
      "Epoch 49/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0049 - val_loss: 0.0069\n",
      "Epoch 50/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0096 - val_loss: 0.0028\n",
      "Epoch 51/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0066 - val_loss: 0.0078\n",
      "Epoch 52/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0060\n",
      "Epoch 53/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0092 - val_loss: 0.0044\n",
      "Epoch 54/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0145 - val_loss: 0.0026\n",
      "Epoch 55/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0072 - val_loss: 0.0021\n",
      "Epoch 56/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0022\n",
      "Epoch 57/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0085 - val_loss: 0.0049\n",
      "Epoch 58/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0089 - val_loss: 0.0065\n",
      "Epoch 59/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0083 - val_loss: 0.0015\n",
      "Epoch 60/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0150 - val_loss: 0.0249\n",
      "Epoch 61/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0217 - val_loss: 0.0022\n",
      "Epoch 62/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0175 - val_loss: 0.0016\n",
      "Epoch 63/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0029 - val_loss: 9.8280e-04\n",
      "Epoch 64/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 65/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 66/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0060 - val_loss: 0.0031\n",
      "Epoch 67/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0017\n",
      "Epoch 68/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0065 - val_loss: 0.0015\n",
      "Epoch 69/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0051 - val_loss: 0.0056\n",
      "Epoch 70/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0145 - val_loss: 0.0010\n",
      "Epoch 71/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0076 - val_loss: 0.0012\n",
      "Epoch 72/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0110 - val_loss: 0.0126\n",
      "Epoch 73/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 0.0022\n",
      "Epoch 74/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 9.4375e-04\n",
      "Epoch 75/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 76/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0036 - val_loss: 7.6899e-04\n",
      "Epoch 77/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0031 - val_loss: 0.0011\n",
      "Epoch 78/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0060 - val_loss: 0.0011\n",
      "Epoch 79/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0065 - val_loss: 0.0013\n",
      "Epoch 80/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0018\n",
      "Epoch 81/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0056 - val_loss: 0.0012\n",
      "Epoch 82/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0103 - val_loss: 7.4093e-04\n",
      "Epoch 83/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0017\n",
      "Epoch 84/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0147 - val_loss: 0.0013\n",
      "Epoch 85/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 7.9226e-04\n",
      "Epoch 86/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0077 - val_loss: 0.0023\n",
      "Epoch 87/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0078 - val_loss: 0.0011\n",
      "Epoch 88/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0011\n",
      "Epoch 89/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0070 - val_loss: 0.0029\n",
      "Epoch 90/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0096 - val_loss: 0.0040\n",
      "Epoch 91/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0046 - val_loss: 0.0010\n",
      "Epoch 92/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0071 - val_loss: 8.7539e-04\n",
      "Epoch 93/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 9.6785e-04\n",
      "Epoch 94/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0060 - val_loss: 8.7143e-04\n",
      "Epoch 95/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0060 - val_loss: 0.0014\n",
      "Epoch 96/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0019\n",
      "Epoch 97/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0096 - val_loss: 0.0026\n",
      "Epoch 98/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0032\n",
      "Epoch 99/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 100/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0433 - val_loss: 0.0014\n",
      "211/211 [==============================] - 0s 581us/step\n",
      "Training RNN model...\n",
      "169/169 [==============================] - 0s 1ms/step\n",
      "43/43 [==============================] - 0s 1ms/step\n",
      "Training LSTM model...\n",
      "169/169 [==============================] - 1s 2ms/step\n",
      "43/43 [==============================] - 0s 2ms/step\n",
      "Training GRU model...\n",
      "169/169 [==============================] - 0s 2ms/step\n",
      "43/43 [==============================] - 0s 2ms/step\n",
      "RNN model:\n",
      " MSE (Train): 123611.84\n",
      " MSE (Test): 133106.51\n",
      " R2 (Train): 0.59\n",
      " R2 (Test): 0.54\n",
      "LSTM model:\n",
      " MSE (Train): 113947.92\n",
      " MSE (Test): 128279.40\n",
      " R2 (Train): 0.62\n",
      " R2 (Test): 0.56\n",
      "GRU model:\n",
      " MSE (Train): 110169.37\n",
      " MSE (Test): 126145.55\n",
      " R2 (Train): 0.63\n",
      " R2 (Test): 0.57\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, SimpleRNN, LSTM, GRU, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"swiss_no_outliers_and_cleaned.csv\")\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[[\"Rooms\", \"Footage\", \"Distance_to_City_Center(km)\", \"Address_Latitude\", \"Address_Longitude\"]]\n",
    "y = df[\"Rent\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Define the input layer\n",
    "input_data = Input(shape=(5,))\n",
    "\n",
    "# Define the encoder\n",
    "encoded = Dense(64, activation='relu')(input_data)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "encoded = Dense(16, activation='relu')(encoded)\n",
    "\n",
    "# Define the bottleneck (compressed representation)\n",
    "bottleneck = Dense(8, activation='relu')(encoded)\n",
    "\n",
    "# Define the decoder\n",
    "decoded = Dense(16, activation='relu')(bottleneck)\n",
    "decoded = Dense(32, activation='relu')(decoded)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "\n",
    "# Define the output layer\n",
    "output_data = Dense(5, activation='linear')(decoded)\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = Model(input_data, output_data)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder model\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Extract the encoder part of the autoencoder\n",
    "encoder = Model(input_data, bottleneck)\n",
    "\n",
    "# Generate a lower-dimensional representation of your input data\n",
    "X_encoded = encoder.predict(X_scaled)\n",
    "\n",
    "# Reshape the encoded data into a suitable format for RNNs, LSTMs, or GRUs\n",
    "timesteps = 2\n",
    "features = 4\n",
    "X_encoded_reshaped = X_encoded.reshape((-1, timesteps, features))\n",
    "\n",
    "# Split the reshaped data into training and testing sets\n",
    "X_encoded_train, X_encoded_test, y_train, y_test = train_test_split(X_encoded_reshaped, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def create_model(model_type, units):\n",
    "    model = Sequential()\n",
    "\n",
    "    if model_type == \"RNN\":\n",
    "        model.add(SimpleRNN(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(SimpleRNN(units, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "    elif model_type == \"LSTM\":\n",
    "        model.add(LSTM(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(units, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "    elif model_type == \"GRU\":\n",
    "        model.add(GRU(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(GRU(units, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "# Train the models and compare their performance\n",
    "model_types = [\"RNN\", \"LSTM\", \"GRU\"]\n",
    "units = 128\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"Training {model_type} model...\")\n",
    "    model = create_model(model_type, units)\n",
    "    \n",
    "    # Add early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_encoded_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    y_pred_train = model.predict(X_encoded_train)\n",
    "    y_pred_test = model.predict(X_encoded_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    results[model_type] = {\n",
    "        \"mse_train\": mse_train,\n",
    "        \"mse_test\": mse_test,\n",
    "        \"r2_train\": r2_train,\n",
    "        \"r2_test\": r2_test,\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "for model_type, result in results.items():\n",
    "    print(f\"{model_type} model:\")\n",
    "    print(f\" MSE (Train): {result['mse_train']:.2f}\")\n",
    "    print(f\" MSE (Test): {result['mse_test']:.2f}\")\n",
    "    print(f\" R2 (Train): {result['r2_train']:.2f}\")\n",
    "    print(f\" R2 (Test): {result['r2_test']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "169/169 [==============================] - 1s 2ms/step - loss: 0.6059 - val_loss: 0.1885\n",
      "Epoch 2/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1618 - val_loss: 0.0420\n",
      "Epoch 3/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0765 - val_loss: 0.0464\n",
      "Epoch 4/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0542 - val_loss: 0.0314\n",
      "Epoch 5/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0725 - val_loss: 0.0155\n",
      "Epoch 6/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0491 - val_loss: 0.0105\n",
      "Epoch 7/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0187 - val_loss: 0.0110\n",
      "Epoch 8/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0235 - val_loss: 0.0082\n",
      "Epoch 9/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0103 - val_loss: 0.0075\n",
      "Epoch 10/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0109 - val_loss: 0.0058\n",
      "Epoch 11/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0317 - val_loss: 0.0099\n",
      "Epoch 12/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0705 - val_loss: 0.0066\n",
      "Epoch 13/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0093 - val_loss: 0.0031\n",
      "Epoch 14/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0220 - val_loss: 0.0057\n",
      "Epoch 15/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0492 - val_loss: 0.0820\n",
      "Epoch 16/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1172 - val_loss: 0.0139\n",
      "Epoch 17/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0325 - val_loss: 0.0036\n",
      "Epoch 18/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0185 - val_loss: 0.0053\n",
      "Epoch 19/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0301 - val_loss: 0.0033\n",
      "Epoch 20/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0147 - val_loss: 0.0074\n",
      "Epoch 21/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0366 - val_loss: 0.0053\n",
      "Epoch 22/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0196 - val_loss: 0.0057\n",
      "Epoch 23/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0332 - val_loss: 0.0313\n",
      "Epoch 24/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0191 - val_loss: 0.0034\n",
      "Epoch 25/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0348 - val_loss: 0.0148\n",
      "Epoch 26/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0181 - val_loss: 0.0043\n",
      "Epoch 27/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0270 - val_loss: 0.0076\n",
      "Epoch 28/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 0.0206\n",
      "Epoch 29/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0550 - val_loss: 0.0033\n",
      "Epoch 30/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0050 - val_loss: 0.0023\n",
      "Epoch 31/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0068 - val_loss: 0.0029\n",
      "Epoch 32/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0101 - val_loss: 0.0052\n",
      "Epoch 33/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0203 - val_loss: 0.0024\n",
      "Epoch 34/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0199 - val_loss: 0.0289\n",
      "Epoch 35/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0772 - val_loss: 0.0044\n",
      "Epoch 36/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0073 - val_loss: 0.0032\n",
      "Epoch 37/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0082\n",
      "Epoch 38/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0100 - val_loss: 0.0145\n",
      "Epoch 39/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0312 - val_loss: 0.0076\n",
      "Epoch 40/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0614 - val_loss: 0.0078\n",
      "Epoch 41/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0865 - val_loss: 0.0298\n",
      "Epoch 42/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0268 - val_loss: 0.0116\n",
      "Epoch 43/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0188 - val_loss: 0.0053\n",
      "Epoch 44/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0824 - val_loss: 0.0712\n",
      "Epoch 45/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0037\n",
      "Epoch 46/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0068 - val_loss: 0.0033\n",
      "Epoch 47/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0092 - val_loss: 0.0058\n",
      "Epoch 48/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0247 - val_loss: 0.0075\n",
      "Epoch 49/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0331 - val_loss: 0.0031\n",
      "Epoch 50/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0030\n",
      "Epoch 51/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0037 - val_loss: 0.0040\n",
      "Epoch 52/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 53/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0112 - val_loss: 0.0062\n",
      "Epoch 54/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0040\n",
      "Epoch 55/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0407 - val_loss: 0.0104\n",
      "Epoch 56/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0061 - val_loss: 0.0025\n",
      "Epoch 57/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0071 - val_loss: 0.0024\n",
      "Epoch 58/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0109 - val_loss: 0.0054\n",
      "Epoch 59/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0282 - val_loss: 0.0029\n",
      "Epoch 60/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0031\n",
      "Epoch 61/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0366 - val_loss: 0.0027\n",
      "Epoch 62/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0055 - val_loss: 0.0078\n",
      "Epoch 63/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0022\n",
      "Epoch 64/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0177 - val_loss: 0.0105\n",
      "Epoch 65/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0415 - val_loss: 0.0139\n",
      "Epoch 66/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0094 - val_loss: 0.0029\n",
      "Epoch 67/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0101 - val_loss: 0.0029\n",
      "Epoch 68/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0084\n",
      "Epoch 69/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0370 - val_loss: 0.0095\n",
      "Epoch 70/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0036\n",
      "Epoch 71/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0233 - val_loss: 0.0175\n",
      "Epoch 72/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0083\n",
      "Epoch 73/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0290 - val_loss: 0.0034\n",
      "Epoch 74/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0112 - val_loss: 0.0025\n",
      "Epoch 75/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0186 - val_loss: 0.0030\n",
      "Epoch 76/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0024\n",
      "Epoch 77/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0020\n",
      "Epoch 78/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0055 - val_loss: 0.0026\n",
      "Epoch 79/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0157 - val_loss: 0.0028\n",
      "Epoch 80/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0181\n",
      "Epoch 81/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0389 - val_loss: 0.0023\n",
      "Epoch 82/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0019\n",
      "Epoch 83/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0079 - val_loss: 0.0101\n",
      "Epoch 84/100\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0034\n",
      "Epoch 85/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0304 - val_loss: 0.0039\n",
      "Epoch 86/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0072 - val_loss: 0.0024\n",
      "Epoch 87/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0062\n",
      "Epoch 88/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0021\n",
      "Epoch 89/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0293 - val_loss: 0.0053\n",
      "Epoch 90/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0080 - val_loss: 0.0048\n",
      "Epoch 91/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0177 - val_loss: 0.0039\n",
      "Epoch 92/100\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0030\n",
      "Epoch 93/100\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0070\n",
      "Epoch 94/100\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 0.0030\n",
      "Epoch 95/100\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0022\n",
      "Epoch 96/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0041\n",
      "Epoch 97/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0227 - val_loss: 0.0019\n",
      "Epoch 98/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0069 - val_loss: 0.0031\n",
      "Epoch 99/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0187 - val_loss: 0.0033\n",
      "Epoch 100/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0142 - val_loss: 0.0020\n",
      "211/211 [==============================] - 0s 614us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remy\\AppData\\Local\\Temp\\ipykernel_9116\\3555402055.py:98: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasRegressor(build_fn=create_model, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, SimpleRNN, LSTM, GRU, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"swiss_no_outliers_and_cleaned.csv\")\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[[\"Rooms\", \"Footage\", \"Distance_to_City_Center(km)\", \"Address_Latitude\", \"Address_Longitude\"]]\n",
    "y = df[\"Rent\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Define the input layer\n",
    "input_data = Input(shape=(5,))\n",
    "\n",
    "# Define the encoder\n",
    "encoded = Dense(64, activation='relu')(input_data)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "encoded = Dense(16, activation='relu')(encoded)\n",
    "\n",
    "# Define the bottleneck (compressed representation)\n",
    "bottleneck = Dense(8, activation='relu')(encoded)\n",
    "\n",
    "# Define the decoder\n",
    "decoded = Dense(16, activation='relu')(bottleneck)\n",
    "decoded = Dense(32, activation='relu')(decoded)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "\n",
    "# Define the output layer\n",
    "output_data = Dense(5, activation='linear')(decoded)\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = Model(input_data, output_data)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder model\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Extract the encoder part of the autoencoder\n",
    "encoder = Model(input_data, bottleneck)\n",
    "\n",
    "# Generate a lower-dimensional representation of your input data\n",
    "X_encoded = encoder.predict(X_scaled)\n",
    "\n",
    "# Reshape the encoded data into a suitable format for RNNs, LSTMs, or GRUs\n",
    "timesteps = 2\n",
    "features = 4\n",
    "X_encoded_reshaped = X_encoded.reshape((-1, timesteps, features))\n",
    "\n",
    "# Split the reshaped data into training and testing sets\n",
    "X_encoded_train, X_encoded_test, y_train, y_test = train_test_split(X_encoded_reshaped, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Modify the create_model function to accept hyperparameters as arguments\n",
    "def create_model(model_type=\"LSTM\", units=128, dropout_rate=0.2, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "\n",
    "    if model_type == \"RNN\":\n",
    "        model.add(SimpleRNN(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(SimpleRNN(units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    elif model_type == \"LSTM\":\n",
    "        model.add(LSTM(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(LSTM(units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    elif model_type == \"GRU\":\n",
    "        model.add(GRU(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(GRU(units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "# Wrap the create_model function in KerasRegressor\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'model_type': ['RNN', 'LSTM', 'GRU'],\n",
    "    'units': [64, 128, 256],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [100, 300, 500],\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_result = grid.fit(X_encoded_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best parameters found: \", grid_result.best_params_)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = grid_result.best_estimator_.model\n",
    "best_model.save(\"best_model.h5\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = best_model.predict(X_encoded_train)\n",
    "y_pred_test = best_model.predict(X_encoded_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"Best model:\")\n",
    "print(f\" MSE (Train): {mse_train:.2f}\")\n",
    "print(f\" MSE (Test): {mse_test:.2f}\")\n",
    "print(f\" R2 (Train): {r2_train:.2f}\")\n",
    "print(f\" R2 (Test): {r2_test:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "169/169 [==============================] - 1s 2ms/step - loss: 0.5153 - val_loss: 0.2873\n",
      "Epoch 2/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1668 - val_loss: 0.3310\n",
      "Epoch 3/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.2086 - val_loss: 0.0963\n",
      "Epoch 4/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1537 - val_loss: 0.0259\n",
      "Epoch 5/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1097 - val_loss: 0.0319\n",
      "Epoch 6/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0651 - val_loss: 0.0172\n",
      "Epoch 7/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0694 - val_loss: 0.0219\n",
      "Epoch 8/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1307 - val_loss: 0.1261\n",
      "Epoch 9/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0573 - val_loss: 0.0118\n",
      "Epoch 10/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0520 - val_loss: 0.0225\n",
      "Epoch 11/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0593 - val_loss: 0.0108\n",
      "Epoch 12/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1214 - val_loss: 0.0102\n",
      "Epoch 13/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0074\n",
      "Epoch 14/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0142 - val_loss: 0.0077\n",
      "Epoch 15/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0262 - val_loss: 0.0479\n",
      "Epoch 16/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0383 - val_loss: 0.0392\n",
      "Epoch 17/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0873 - val_loss: 0.0047\n",
      "Epoch 18/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0095 - val_loss: 0.0038\n",
      "Epoch 19/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0209 - val_loss: 0.0040\n",
      "Epoch 20/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0437 - val_loss: 0.0046\n",
      "Epoch 21/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0253 - val_loss: 0.0070\n",
      "Epoch 22/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0759 - val_loss: 0.0052\n",
      "Epoch 23/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0620 - val_loss: 0.0077\n",
      "Epoch 24/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0989 - val_loss: 0.0043\n",
      "Epoch 25/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0175 - val_loss: 0.0035\n",
      "Epoch 26/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0238 - val_loss: 0.0102\n",
      "Epoch 27/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0663 - val_loss: 0.0058\n",
      "Epoch 28/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0058\n",
      "Epoch 29/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0080 - val_loss: 0.0026\n",
      "Epoch 30/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0029\n",
      "Epoch 31/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0392 - val_loss: 0.0025\n",
      "Epoch 32/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0261 - val_loss: 0.0058\n",
      "Epoch 33/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0704 - val_loss: 0.0027\n",
      "Epoch 34/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0044 - val_loss: 0.0021\n",
      "Epoch 35/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0075 - val_loss: 0.0025\n",
      "Epoch 36/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0221 - val_loss: 0.0173\n",
      "Epoch 37/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0269 - val_loss: 0.0030\n",
      "Epoch 38/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0080 - val_loss: 0.0023\n",
      "Epoch 39/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0063 - val_loss: 0.0038\n",
      "Epoch 40/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0049 - val_loss: 0.0051\n",
      "Epoch 41/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0067\n",
      "Epoch 42/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0023\n",
      "Epoch 43/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1230 - val_loss: 0.0073\n",
      "Epoch 44/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1228 - val_loss: 0.0036\n",
      "Epoch 45/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0421 - val_loss: 0.0050\n",
      "Epoch 46/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1217 - val_loss: 0.0079\n",
      "Epoch 47/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1576 - val_loss: 0.0075\n",
      "Epoch 48/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1031 - val_loss: 0.0027\n",
      "Epoch 49/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0573 - val_loss: 0.0045\n",
      "Epoch 50/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0147 - val_loss: 0.0030\n",
      "Epoch 51/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0041\n",
      "Epoch 52/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0308 - val_loss: 0.0103\n",
      "Epoch 53/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0183 - val_loss: 0.0062\n",
      "Epoch 54/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0380 - val_loss: 0.0143\n",
      "Epoch 55/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0026\n",
      "Epoch 56/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0048\n",
      "Epoch 57/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0217 - val_loss: 0.0030\n",
      "Epoch 58/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0518 - val_loss: 0.0018\n",
      "Epoch 59/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0051 - val_loss: 0.0131\n",
      "Epoch 60/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0196 - val_loss: 0.0059\n",
      "Epoch 61/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0093 - val_loss: 0.0158\n",
      "Epoch 62/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0225 - val_loss: 0.0041\n",
      "Epoch 63/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0219 - val_loss: 0.0018\n",
      "Epoch 64/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0707 - val_loss: 0.0053\n",
      "Epoch 65/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0022\n",
      "Epoch 66/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0101 - val_loss: 0.0018\n",
      "Epoch 67/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0150 - val_loss: 0.0018\n",
      "Epoch 68/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0096 - val_loss: 0.0017\n",
      "Epoch 69/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0091 - val_loss: 0.0016\n",
      "Epoch 70/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0164\n",
      "Epoch 71/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0823 - val_loss: 0.0018\n",
      "Epoch 72/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.0019\n",
      "Epoch 73/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0091 - val_loss: 0.0034\n",
      "Epoch 74/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0364 - val_loss: 0.0061\n",
      "Epoch 75/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0357 - val_loss: 0.0044\n",
      "Epoch 76/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0895 - val_loss: 0.0042\n",
      "Epoch 77/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0205 - val_loss: 0.0025\n",
      "Epoch 78/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0195 - val_loss: 0.0059\n",
      "Epoch 79/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0055\n",
      "Epoch 80/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 81/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0073 - val_loss: 0.0057\n",
      "Epoch 82/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0194 - val_loss: 0.0096\n",
      "Epoch 83/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0213 - val_loss: 0.0145\n",
      "Epoch 84/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0599 - val_loss: 0.0055\n",
      "Epoch 85/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0028 - val_loss: 0.0011\n",
      "Epoch 86/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0090 - val_loss: 0.0030\n",
      "Epoch 87/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0214 - val_loss: 0.0083\n",
      "Epoch 88/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0605 - val_loss: 0.0060\n",
      "Epoch 89/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0057 - val_loss: 0.0024\n",
      "Epoch 90/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 91/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0052 - val_loss: 0.0116\n",
      "Epoch 92/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0104 - val_loss: 0.0026\n",
      "Epoch 93/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0366 - val_loss: 0.0027\n",
      "Epoch 94/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0365 - val_loss: 0.0019\n",
      "Epoch 95/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0763 - val_loss: 0.0021\n",
      "Epoch 96/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0018\n",
      "Epoch 97/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0173 - val_loss: 0.0177\n",
      "Epoch 98/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0591 - val_loss: 0.0018\n",
      "Epoch 99/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 100/100\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0011\n",
      "211/211 [==============================] - 0s 601us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remy\\AppData\\Local\\Temp\\ipykernel_11928\\1332695565.py:103: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasRegressor(build_fn=create_model, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'units': 128, 'model_type': 'LSTM', 'learning_rate': 0.1, 'epochs': 50, 'dropout_rate': 0.1, 'batch_size': 64}\n",
      "169/169 [==============================] - 0s 1ms/step\n",
      "43/43 [==============================] - 0s 2ms/step\n",
      "Best model:\n",
      " MSE (Train): 2003475.92\n",
      " MSE (Test): 1952781.90\n",
      " R2 (Train): -5.68\n",
      " R2 (Test): -5.72\n"
     ]
    }
   ],
   "source": [
    "##Version mit RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, SimpleRNN, LSTM, GRU, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"swiss_no_outliers_and_cleaned.csv\")\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[[\"Rooms\", \"Footage\", \"Distance_to_City_Center(km)\", \"Address_Latitude\", \"Address_Longitude\"]]\n",
    "y = df[\"Rent\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Define the input layer\n",
    "input_data = Input(shape=(5,))\n",
    "\n",
    "# Define the encoder\n",
    "encoded = Dense(64, activation='relu')(input_data)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "encoded = Dense(16, activation='relu')(encoded)\n",
    "\n",
    "# Define the bottleneck (compressed representation)\n",
    "bottleneck = Dense(8, activation='relu')(encoded)\n",
    "\n",
    "# Define the decoder\n",
    "decoded = Dense(16, activation='relu')(bottleneck)\n",
    "decoded = Dense(32, activation='relu')(decoded)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "\n",
    "# Define the output layer\n",
    "output_data = Dense(5, activation='linear')(decoded)\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = Model(input_data, output_data)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder model\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Extract the encoder part of the autoencoder\n",
    "encoder = Model(input_data, bottleneck)\n",
    "\n",
    "# Generate a lower-dimensional representation of your input data\n",
    "X_encoded = encoder.predict(X_scaled)\n",
    "\n",
    "# Reshape the encoded data into a suitable format for RNNs, LSTMs, or GRUs\n",
    "timesteps = 2\n",
    "features = 4\n",
    "X_encoded_reshaped = X_encoded.reshape((-1, timesteps, features))\n",
    "\n",
    "# Split the reshaped data into training and testing sets\n",
    "X_encoded_train, X_encoded_test, y_train, y_test = train_test_split(X_encoded_reshaped, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Modify the create_model function to accept hyperparameters as arguments\n",
    "def create_model(model_type=\"LSTM\", units=128, dropout_rate=0.2, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "\n",
    "    if model_type == \"RNN\":\n",
    "        model.add(SimpleRNN(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(SimpleRNN(units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    elif model_type == \"LSTM\":\n",
    "        model.add(LSTM(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(LSTM(units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    elif model_type == \"GRU\":\n",
    "        model.add(GRU(units, input_shape=(timesteps, features), activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(GRU(units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Wrap the create_model function in KerasRegressor\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'model_type': ['RNN', 'LSTM', 'GRU'],\n",
    "    'units': [64, 128, 256, 512, 1024],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'learning_rate': [0.05, 0.01, 0.001, 0.0001],\n",
    "    'batch_size': [16, 32, 64, 128],\n",
    "    'epochs': [50, 100, 200],\n",
    "}\n",
    "\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, n_jobs=-1, cv=3, scoring='neg_mean_squared_error', random_state=42)\n",
    "\n",
    "# Fit the random search to the data\n",
    "random_search_result = random_search.fit(X_encoded_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best parameters found: \", random_search_result.best_params_)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = random_search_result.best_estimator_.model\n",
    "best_model.save(\"best_model.h5\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = best_model.predict(X_encoded_train)\n",
    "y_pred_test = best_model.predict(X_encoded_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"Best model:\")\n",
    "print(f\" MSE (Train): {mse_train:.2f}\")\n",
    "print(f\" MSE (Test): {mse_test:.2f}\")\n",
    "print(f\" R2 (Train): {r2_train:.2f}\")\n",
    "print(f\" R2 (Test): {r2_test:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
